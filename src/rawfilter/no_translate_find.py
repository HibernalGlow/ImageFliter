import sys
import os
import logging
logging.basicConfig()  # åˆå§‹åŒ–æ ‡å‡†åº“logging

# æ ‡å‡†åº“å¯¼å…¥
import re
import shutil
from datetime import datetime
import argparse
import io
import functools
import subprocess
import threading
from functools import partial
import random
import zipfile
import win32com.client  # ç”¨äºåˆ›å»ºå¿«æ·æ–¹å¼

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import pyperclip
from PIL import Image
import pillow_avif
import pillow_jxl
from pathlib import Path
from colorama import init, Fore, Style
from typing import List, Dict, Set, Tuple, Optional, Union
from opencc import OpenCC  # ç”¨äºç¹ç®€è½¬æ¢
from concurrent.futures import ThreadPoolExecutor, as_completed
from nodes.pics.hash.calculate_hash_custom import ImageClarityEvaluator
from textual_logger import TextualLoggerManager
from nodes.utils.number_shortener import shorten_number_cn
from nodes.tui.mode_manager import create_mode_manager
import json
from nodes.pics.filter.group_analyzer import GroupAnalyzer

from loguru import logger
import os
import sys
from pathlib import Path
from datetime import datetime

def setup_logger(app_name="app", project_root=None, console_output=True):
    """é…ç½® Loguru æ—¥å¿—ç³»ç»Ÿ
    
    Args:
        app_name: åº”ç”¨åç§°ï¼Œç”¨äºæ—¥å¿—ç›®å½•
        project_root: é¡¹ç›®æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
        console_output: æ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œé»˜è®¤ä¸ºTrue
        
    Returns:
        tuple: (logger, config_info)
            - logger: é…ç½®å¥½çš„ logger å®ä¾‹
            - config_info: åŒ…å«æ—¥å¿—é…ç½®ä¿¡æ¯çš„å­—å…¸
    """
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    if project_root is None:
        project_root = Path(__file__).parent.resolve()
    
    # æ¸…é™¤é»˜è®¤å¤„ç†å™¨
    logger.remove()
    
    # æœ‰æ¡ä»¶åœ°æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ï¼ˆç®€æ´ç‰ˆæ ¼å¼ï¼‰
    if console_output:
        logger.add(
            sys.stdout,
            level="INFO",
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <blue>{elapsed}</blue> | <level>{level.icon} {level: <8}</level> | <cyan>{name}:{function}:{line}</cyan> - <level>{message}</level>"
        )
    
    # ä½¿ç”¨ datetime æ„å»ºæ—¥å¿—è·¯å¾„
    current_time = datetime.now()
    date_str = current_time.strftime("%Y-%m-%d")
    hour_str = current_time.strftime("%H")
    minute_str = current_time.strftime("%M%S")
    
    # æ„å»ºæ—¥å¿—ç›®å½•å’Œæ–‡ä»¶è·¯å¾„
    log_dir = os.path.join(project_root, "logs", app_name, date_str, hour_str)
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"{minute_str}.log")
    
    # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    logger.add(
        log_file,
        level="DEBUG",
        rotation="10 MB",
        retention="30 days",
        compression="zip",
        encoding="utf-8",
        format="{time:YYYY-MM-DD HH:mm:ss} | {elapsed} | {level.icon} {level: <8} | {name}:{function}:{line} - {message}",
    )
    
    # åˆ›å»ºé…ç½®ä¿¡æ¯å­—å…¸
    config_info = {
        'log_file': log_file,
    }
    
    logger.info(f"æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œåº”ç”¨åç§°: {app_name}")
    return logger, config_info

logger, config_info = setup_logger(app_name="no_translate_find", console_output=True)

    # è®¾ç½®Textualæ—¥å¿—å¸ƒå±€
TEXTUAL_LAYOUT = {
    "stats": {
        "ratio": 2,
        "title": "ğŸ“Š å¤„ç†ç»Ÿè®¡",
        "style": "lightgreen"
    },
    "process": {
        "ratio": 2, 
        "title": "ğŸ”„ è¿›åº¦",
        "style": "lightcyan",
    },
    "file_ops": {
        "ratio": 3,
        "title": "ğŸ“‚ æ–‡ä»¶æ“ä½œ",
        "style": "lightblue",
    },
    "group_info": {
        "ratio": 3,
        "title": "ğŸ” ç»„å¤„ç†ä¿¡æ¯",
        "style": "lightpink",
    },
    "error_log": {
        "ratio": 2,
        "title": "âš ï¸ é”™è¯¯æ—¥å¿—",
        "style": "lightred",
    }
}

class ReportGenerator:
    """ç”Ÿæˆå¤„ç†æŠ¥å‘Šçš„ç±»"""
    def __init__(self):
        self.report_sections = []
        self.stats = {
            'total_files': 0,
            'total_groups': 0,
            'moved_to_trash': 0,
            'moved_to_multi': 0,
            'skipped_files': 0,
            'created_shortcuts': 0
        }
        self.group_details = []
        
    def add_group_detail(self, group_name: str, details: Dict):
        """æ·»åŠ ç»„å¤„ç†è¯¦æƒ…"""
        self.group_details.append({
            'name': group_name,
            'details': details
        })
        
    def update_stats(self, key: str, value: int = 1):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats[key] = self.stats.get(key, 0) + value
        
    def add_section(self, title: str, content: str):
        """æ·»åŠ æŠ¥å‘Šç« èŠ‚"""
        self.report_sections.append({
            'title': title,
            'content': content
        })
        
    def generate_report(self, base_dir: str) -> str:
        """ç”Ÿæˆæœ€ç»ˆçš„MDæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report = [
            f"# æ–‡ä»¶å¤„ç†æŠ¥å‘Š",
            f"ç”Ÿæˆæ—¶é—´: {timestamp}",
            f"å¤„ç†ç›®å½•: {base_dir}",
            "",
            "## å¤„ç†ç»Ÿè®¡",
            f"- æ€»æ–‡ä»¶æ•°: {shorten_number_cn(self.stats['total_files'])}",
            f"- æ€»åˆ†ç»„æ•°: {shorten_number_cn(self.stats['total_groups'])}",
            f"- ç§»åŠ¨åˆ°trashç›®å½•: {shorten_number_cn(self.stats['moved_to_trash'])}",
            f"- ç§»åŠ¨åˆ°multiç›®å½•: {shorten_number_cn(self.stats['moved_to_multi'])}",
            f"- è·³è¿‡çš„æ–‡ä»¶: {shorten_number_cn(self.stats['skipped_files'])}",
            f"- åˆ›å»ºçš„å¿«æ·æ–¹å¼: {shorten_number_cn(self.stats['created_shortcuts'])}",
            ""
        ]
        
        # æ·»åŠ ç»„è¯¦æƒ…ï¼ˆæ”¹ä¸ºåˆ—è¡¨å½¢å¼ï¼‰
        if self.group_details:
            report.append("## å¤„ç†è¯¦æƒ…åˆ—è¡¨")
            for group in self.group_details:
                report.append(f"- **{group['name']}**")
                details = group['details']
                if 'chinese_versions' in details:
                    report.append("  - æ±‰åŒ–ç‰ˆæœ¬:")
                    for file in details['chinese_versions']:
                        report.append(f"    - {file}")
                if 'other_versions' in details:
                    report.append("  - å…¶ä»–ç‰ˆæœ¬:")
                    for file in details['other_versions']:
                        report.append(f"    - {file}")
                if 'actions' in details:
                    report.append("  - æ‰§è¡Œæ“ä½œ:")
                    for action in details['actions']:
                        report.append(f"    - {action}")
                report.append("")  # ç»„é—´ç©ºè¡Œ
        
        # å…¶ä»–ç« èŠ‚ä¿æŒæ ‡é¢˜å½¢å¼
        for section in self.report_sections:
            report.append(f"## {section['title']}")
            report.append(section['content'])
            report.append("")
            
        return "\n".join(report)
        
    def save_report(self, base_dir: str, filename: Optional[str] = None):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if filename is None:
            filename = f"å¤„ç†æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        report_path = os.path.join(base_dir, filename)
        report_content = self.generate_report(base_dir)
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            return report_path
        except Exception as e:
            logger.error("[#error_log] âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: %s", str(e))
            logger.exception("[#error_log] å¼‚å¸¸å †æ ˆ:")  # è‡ªåŠ¨è®°å½•å †æ ˆä¿¡æ¯
            # åœ¨ç•Œé¢æ˜¾ç¤ºé”™è¯¯è¯¦æƒ…
            logger.info("[#process] ğŸ’¥ é‡åˆ°ä¸¥é‡é”™è¯¯ï¼Œè¯·æ£€æŸ¥error_logé¢æ¿")
            return None

# åˆå§‹åŒ–colorama
init()

# æ·»åŠ è‡ªå®šä¹‰æ¨¡å—è·¯å¾„å¹¶å¯¼å…¥
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åˆå§‹åŒ–OpenCC
cc_s2t = OpenCC('s2t')  # åˆ›å»ºç®€ä½“åˆ°ç¹ä½“è½¬æ¢å™¨
cc_t2s = OpenCC('t2s')  # åˆ›å»ºç¹ä½“åˆ°ç®€ä½“è½¬æ¢å™¨

# æ”¯æŒçš„å‹ç¼©åŒ…æ ¼å¼
ARCHIVE_EXTENSIONS = {
    '.zip', '.rar', '.7z', '.cbr', '.cbz', 
    '.cb7', '.cbt', '.tar', '.gz', '.bz2'
}

# æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
IMAGE_EXTENSIONS = {
    '.jpg', '.jpeg', '.png', '.webp', '.avif', '.jxl',
    '.gif', '.bmp', '.tiff', '.tif', '.heic', '.heif'
}

def preprocess_keywords(keywords: Set[str]) -> Set[str]:
    """é¢„å¤„ç†å…³é”®è¯é›†åˆï¼Œæ·»åŠ ç¹ç®€ä½“å˜ä½“"""
    processed = set()
    for keyword in keywords:
        # æ·»åŠ åŸå§‹å…³é”®è¯ï¼ˆå°å†™ï¼‰
        processed.add(keyword.lower())
        # æ·»åŠ ç¹ä½“ç‰ˆæœ¬
        traditional = cc_s2t.convert(keyword)
        processed.add(traditional.lower())
        # æ·»åŠ ç®€ä½“ç‰ˆæœ¬
        simplified = cc_t2s.convert(keyword)
        processed.add(simplified.lower())
    return processed

# é¢„å¤„ç†æ±‰åŒ–ç‰ˆæœ¬å…³é”®è¯é›†åˆ
CHINESE_VERSION_KEYWORDS = {
    'æ±‰åŒ–', 'æ¼¢åŒ–',  # æ±‰åŒ–/æ¼¢åŒ–
    'ç¿»è¯‘', 'ç¿»è¨³', 'ç¿»è­¯', # ç¿»è¯‘ç›¸å…³
    'ä¸­å›½ç¿»è¯‘', 'ä¸­å›½ç¿»è¨³', 'ä¸­å›½èª','chinese','ä¸­æ–‡','ä¸­å›½', # ä¸­æ–‡ç¿»è¯‘
    'åµŒå­—',  # åµŒå­—
    'æƒåœ–', 'æƒ', # æ‰«å›¾ç›¸å…³
    'åˆ¶ä½œ', 'è£½ä½œ', # åˆ¶ä½œç›¸å…³
    'é‡åµŒ',  # é‡æ–°åµŒå…¥
    'ä¸ªäºº', # ä¸ªäººç¿»è¯‘
    'å»ç ',
    'æ—¥è¯­ç¤¾',
    'åˆ¶ä½œ',
    'æœºç¿»',
    'èµåŠ©',
    'æ±‰', 'æ¼¢', # æ±‰å­—ç›¸å…³
    'æ•°ä½', 'æœªæ¥æ•°ä½', 'æ–°è§†ç•Œ', # æ±‰åŒ–ç›¸å…³
    'å‡ºç‰ˆ', 'é’æ–‡å‡ºç‰ˆ', # ç¿»è¯‘ç›¸å…³
    'è„¸è‚¿', 'æ— æ¯’', 'ç©ºæ°—ç³»', 'å¤¢ä¹‹è¡Œè¹¤', 'èŒå¹»é´¿é„‰', 'ç»…å£«ä»“åº“', 'Lolipoi', 'é´ä¸‹','CEå®¶æ—ç¤¾',
    'ä¸å¯è§†', 'ä¸€åŒ™å’–å•¡è±†', 'æ— é‚ªæ°”', 'æ´¨äº”', 'ç™½æ¨', 'ç‘æ ‘',  # å¸¸è§æ±‰åŒ–ç»„å
    'å†Šèªè‰å ‚','æ·«ä¹¦é¦†','æ˜¯å°ç‹ç‹¸å“¦','å·¥æˆ¿','å·¥åŠ','åŸºåœ°'
    'æ±‰åŒ–ç»„', 'æ¼¢åŒ–çµ„', 'æ±‰åŒ–ç¤¾', 'æ¼¢åŒ–ç¤¾', 'CE å®¶æ—ç¤¾', 'CE å®¶æ—ç¤¾',  # å¸¸è§åç¼€
    'ä¸ªäººæ±‰åŒ–', 'å€‹äººæ¼¢åŒ–'  # ä¸ªäººæ±‰åŒ–
}

# é¢„å¤„ç†åŸç‰ˆå…³é”®è¯é›†åˆ
ORIGINAL_VERSION_KEYWORDS = {
    'Digital', 'DLç‰ˆ', 'DL', 'ãƒ‡ã‚¸ã‚¿ãƒ«ç‰ˆ',  # æ•°å­—ç‰ˆæœ¬ç›¸å…³
    'å‡ºç‰ˆ', 'å‡ºç‰ˆç¤¾', 'æ›¸åº—ç‰ˆ', 'é€šå¸¸ç‰ˆ',  # å‡ºç‰ˆç›¸å…³
    'ç„¡ä¿®æ­£', 'æ— ä¿®æ­£', 'æ— ä¿®', 'ç„¡ä¿®',  # æ— ä¿®æ­£ç‰ˆæœ¬
    'å®Œå…¨ç‰ˆ', 'å®Œæ•´ç‰ˆ', # å®Œæ•´ç‰ˆæœ¬
    'å…¨å½©', 'å®˜æ–¹', 'ã‚«ãƒ©ãƒ¼', # å…¨å½©ç‰ˆæœ¬
    'ã‚«ãƒ©ãƒ¼', 'å½©è‰²', 'å½©è‰²ç‰ˆ', 'å½©è‰²ç‰ˆ', # å½©è‰²ç‰ˆæœ¬
}

# é¢„å¤„ç†é»‘åå•å…³é”®è¯é›†åˆ
BLACKLIST_KEYWORDS = {
    # ç”»é›†/å›¾é›†ç›¸å…³
    'trash', 'ç”»é›†', 'ç•«é›†', 'artbook', 'art book', 'art works', 'illustrations',
    'å›¾é›†', 'åœ–é›†', 'illust', 'collection',
    'æ‚å›¾', 'é›œåœ–', 'æ‚å›¾åˆé›†', 'é›œåœ–åˆé›†',
    # å…¶ä»–ä¸éœ€è¦å¤„ç†çš„ç±»å‹
    'pixiv', 'fanbox', 'gumroad', 'twitter',
    'å¾…åˆ†ç±»', 'å¾…è™•ç†', 'å¾…åˆ†é¡',
    'å›¾åŒ…', 'åœ–åŒ…',
    'å›¾ç‰‡', 'åœ–ç‰‡',
    'cg', 'CG',
    '02cos',
}

# é¢„å¤„ç†æ‰€æœ‰å…³é”®è¯é›†åˆ
_CHINESE_VERSION_KEYWORDS_FULL = preprocess_keywords(CHINESE_VERSION_KEYWORDS)
_ORIGINAL_VERSION_KEYWORDS_FULL = preprocess_keywords(ORIGINAL_VERSION_KEYWORDS)
_BLACKLIST_KEYWORDS_FULL = preprocess_keywords(BLACKLIST_KEYWORDS)

# æ·»åŠ çº¿ç¨‹æœ¬åœ°å­˜å‚¨
thread_local = threading.local()

def clean_filename(filename: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œåªä¿ç•™ä¸»æ–‡ä»¶åéƒ¨åˆ†è¿›è¡Œæ¯”è¾ƒ"""
    # ç§»é™¤æ‰©å±•å
    name = os.path.splitext(filename)[0]
    
    # æå–æ‹¬å·ä¸­çš„å†…å®¹
    pattern_brackets = re.compile(r'\[([^\[\]]+)\]')  # åŒ¹é…æ–¹æ‹¬å·
    pattern_parentheses = re.compile(r'\(([^\(\)]+)\)')  # åŒ¹é…åœ†æ‹¬å·
    pattern_curly_brackets = re.compile(r'\{(.*?)\}')  # åŒ¹é…èŠ±æ‹¬å·
    hanhua_match = re.search(r'\[(.*?æ±‰åŒ–.*?)\]', name)
    hanhua_info = hanhua_match.group(1) if hanhua_match else ''
    
    # ç§»é™¤æ‰€æœ‰æ‹¬å·å†…å®¹
    name = pattern_brackets.sub('', name)  # ç§»é™¤æ‰€æœ‰æ–¹æ‹¬å·å†…å®¹
    name = pattern_parentheses.sub('', name)  # ç§»é™¤æ‰€æœ‰åœ†æ‹¬å·å†…å®¹
    name = pattern_curly_brackets.sub('', name)  # ç§»é™¤æ‰€æœ‰èŠ±æ‹¬å·å†…å®¹
    # æ¸…ç†å¤šä½™ç©ºæ ¼
    # name = re.sub(r'\s+', ' ', name)  # å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ª
    name = re.sub(r'\s+', '', name)  # å®Œå…¨å»é™¤æ‰€æœ‰ç©ºæ ¼
    name = name.strip().lower()  # è½¬æ¢ä¸ºå°å†™å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
    
    # è¿”å›æ¸…ç†åçš„åç§°å’Œæ±‰åŒ–ä¿¡æ¯
    return name, hanhua_info

@functools.lru_cache(maxsize=10000)
def is_chinese_version(filename: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ±‰åŒ–ç‰ˆæœ¬"""
    # è½¬æ¢æ–‡ä»¶åä¸ºå°å†™
    filename_lower = filename.lower()
    # ç›´æ¥ä½¿ç”¨é¢„å¤„ç†å¥½çš„å…³é”®è¯é›†åˆè¿›è¡Œæ£€æŸ¥
    return any(keyword in filename_lower for keyword in _CHINESE_VERSION_KEYWORDS_FULL)

@functools.lru_cache(maxsize=10000)
def has_original_keywords(filename: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦åŒ…å«åŸç‰ˆç‰¹æ®Šå…³é”®å­—"""
    # è½¬æ¢æ–‡ä»¶åä¸ºå°å†™
    filename_lower = filename.lower()
    # ç›´æ¥ä½¿ç”¨é¢„å¤„ç†å¥½çš„å…³é”®è¯é›†åˆè¿›è¡Œæ£€æŸ¥
    return any(keyword in filename_lower for keyword in _ORIGINAL_VERSION_KEYWORDS_FULL)

# ä½¿ç”¨ functools.lru_cache è£…é¥°å™¨ç¼“å­˜ç»“æœ
@functools.lru_cache(maxsize=10000)
def is_in_blacklist(filepath: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶åæˆ–è·¯å¾„æ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯"""
    # è½¬æ¢è·¯å¾„ä¸ºå°å†™ï¼Œåªè½¬æ¢ä¸€æ¬¡
    filepath_lower = str(filepath).lower()
    # ç›´æ¥ä½¿ç”¨é¢„å¤„ç†å¥½çš„å…³é”®è¯é›†åˆè¿›è¡Œæ£€æŸ¥
    return any(keyword in filepath_lower for keyword in _BLACKLIST_KEYWORDS_FULL)

def is_besscan_version(filename: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºåˆ¥ã‚¹ã‚­ãƒ£ãƒ³ç‰ˆæœ¬"""
    return 'åˆ¥ã‚¹ã‚­ãƒ£ãƒ³' in filename

def group_similar_files(files: List[str]) -> Dict[str, List[str]]:
    """å°†ç›¸ä¼¼æ–‡ä»¶åˆ†ç»„"""
    groups: Dict[str, List[str]] = {}
    
    for file in files:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨é»‘åå•ä¸­
        if is_in_blacklist(file):
            logger.info("[#file_ops] â­ï¸ è·³è¿‡é»‘åå•æ–‡ä»¶: %s", file)
            continue
            
        clean_name, _ = clean_filename(file)
        if clean_name not in groups:
            groups[clean_name] = []
        groups[clean_name].append(file)
        
    return groups

def get_7zip_path() -> str:
    """è·å–7zipå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„"""
    # å¸¸è§çš„7zipå®‰è£…è·¯å¾„
    possible_paths = [
        r"C:\Program Files\7-Zip\7z.exe",
        r"C:\Program Files (x86)\7-Zip\7z.exe",
        os.path.join(os.getenv("ProgramFiles", ""), "7-Zip", "7z.exe"),
        os.path.join(os.getenv("ProgramFiles(x86)", ""), "7-Zip", "7z.exe"),
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return f'"{path}"'  # ç”¨å¼•å·åŒ…è£¹è·¯å¾„
            
    # å¦‚æœæ‰¾ä¸åˆ°7zipï¼Œå°è¯•ä½¿ç”¨å‘½ä»¤è¡Œçš„7z
    try:
        subprocess.run(['7z'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return '7z'
    except:
        return None

def get_image_count(archive_path: str) -> int:
    """è®¡ç®—å‹ç¼©åŒ…ä¸­çš„å›¾ç‰‡æ€»æ•°"""
    try:
        # å…ˆå°è¯•ä½¿ç”¨zipfileæ¨¡å—
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                # ç»Ÿè®¡æ”¯æŒçš„å›¾ç‰‡æ ¼å¼æ–‡ä»¶æ•°é‡
                count = sum(1 for f in zf.namelist() 
                           if os.path.splitext(f.lower())[1] in IMAGE_EXTENSIONS)
                return count
        except zipfile.BadZipFile:
            # zipfileå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨7z
            if not hasattr(thread_local, '7z_path'):
                thread_local.seven_zip_path = get_7zip_path()
                
            if thread_local.seven_zip_path:
                cmd = f'{thread_local.seven_zip_path} l "{archive_path}"'
                result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                if result.returncode == 0:
                    # è§£æ7zå‘½ä»¤è¾“å‡ºï¼Œè®¡ç®—å›¾ç‰‡æ–‡ä»¶æ•°
                    lines = result.stdout.splitlines()
                    count = 0
                    for line in lines:
                        for ext in IMAGE_EXTENSIONS:
                            if line.lower().strip().endswith(ext):
                                count += 1
                    return count
            
            # å¦‚æœ7zä¹Ÿå¤±è´¥äº†ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›0
            logger.error("[#error_log] âŒ æ— æ³•æ‰“å¼€å‹ç¼©æ–‡ä»¶ %s", archive_path)
            return 0
    except Exception as e:
        logger.error("[#error_log] âŒ ç»Ÿè®¡å›¾ç‰‡æ•°é‡å¤±è´¥ %s: %s", archive_path, str(e))
        return 0
def get_sample_images(archive_path: str, temp_dir: str, sample_count: int = 3) -> List[str]:
    """ä»å‹ç¼©åŒ…ä¸­æå–æ ·æœ¬å›¾ç‰‡åˆ°ä¸´æ—¶ç›®å½•"""
    try:
        with zipfile.ZipFile(archive_path, 'r') as zf:
            # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
            image_files = [(f, info.file_size) for f, info in 
                         ((f, zf.getinfo(f)) for f in zf.namelist())
                         if os.path.splitext(f.lower())[1] in IMAGE_EXTENSIONS]
            
            if not image_files:
                return []
            
            # æŒ‰æ–‡ä»¶å¤§å°æ’åº
            image_files.sort(key=lambda x: x[1], reverse=True)
            
            # é€‰æ‹©æ ·æœ¬
            samples = []
            if image_files:
                samples.append(image_files[0][0])  # æœ€å¤§çš„æ–‡ä»¶
                if len(image_files) > 2:
                    samples.append(image_files[len(image_files)//2][0])  # ä¸­é—´çš„æ–‡ä»¶
                
                # ä»å‰30%é€‰æ‹©å‰©ä½™æ ·æœ¬
                top_30_percent = image_files[:max(3, len(image_files) // 3)]
                while len(samples) < sample_count and top_30_percent:
                    sample = random.choice(top_30_percent)[0]
                    if sample not in samples:
                        samples.append(sample)
            
            # æå–é€‰ä¸­çš„æ ·æœ¬åˆ°ä¸´æ—¶ç›®å½•
            extracted_files = []
            for sample in samples:
                temp_file = os.path.join(temp_dir, os.path.basename(sample))
                with zf.open(sample) as source, open(temp_file, 'wb') as target:
                    shutil.copyfileobj(source, target)
                if os.path.exists(temp_file):
                    extracted_files.append(temp_file)
                    
            return extracted_files
            
    except Exception as e:
        logger.error("[#error_log] âŒ æå–æ ·æœ¬å›¾ç‰‡å¤±è´¥ %s: %s", archive_path, str(e))
        return []

def calculate_representative_width(archive_path: str, sample_count: int = 3) -> int:
    """è®¡ç®—å‹ç¼©åŒ…ä¸­å›¾ç‰‡çš„ä»£è¡¨å®½åº¦ï¼ˆä½¿ç”¨æŠ½æ ·å’Œä¸­ä½æ•°ï¼‰"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        ext = os.path.splitext(archive_path)[1].lower()
        if ext not in {'.zip', '.cbz'}:  # åªå¤„ç†zipæ ¼å¼
            return 0

        # è·å–å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶ä¿¡æ¯
        image_files = []
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for info in zf.infolist():
                    if os.path.splitext(info.filename.lower())[1] in IMAGE_EXTENSIONS:
                        image_files.append((info.filename, info.file_size))
        except zipfile.BadZipFile:
            logger.info("[#error_log] âš ï¸ æ— æ•ˆçš„ZIPæ–‡ä»¶: %s", archive_path)
            return 0

        if not image_files:
            return 0

        # æŒ‰æ–‡ä»¶å¤§å°æ’åº
        image_files.sort(key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©æ ·æœ¬
        samples = []
        if image_files:
            samples.append(image_files[0][0])  # æœ€å¤§çš„æ–‡ä»¶
            if len(image_files) > 2:
                samples.append(image_files[len(image_files)//2][0])  # ä¸­é—´çš„æ–‡ä»¶
            
            # ä»å‰30%é€‰æ‹©å‰©ä½™æ ·æœ¬
            top_30_percent = image_files[:max(3, len(image_files) // 3)]
            while len(samples) < sample_count and top_30_percent:
                sample = random.choice(top_30_percent)[0]
                if sample not in samples:
                    samples.append(sample)

        widths = []
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for sample in samples:
                    try:
                        # ç›´æ¥ä»zipè¯»å–åˆ°å†…å­˜
                        with zf.open(sample) as file:
                            img_data = file.read()
                            with Image.open(io.BytesIO(img_data)) as img:
                                widths.append(img.width)
                    except Exception as e:
                        logger.info("[#error_log] âš ï¸ è¯»å–å›¾ç‰‡å®½åº¦å¤±è´¥ %s: %s", sample, str(e))
                        continue
        except Exception as e:
            logger.info("[#error_log] âš ï¸ æ‰“å¼€ZIPæ–‡ä»¶å¤±è´¥: %s", str(e))
            return 0

        if not widths:
            return 0

        # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºä»£è¡¨å®½åº¦
        return int(sorted(widths)[len(widths)//2])

    except Exception as e:
        logger.info("[#error_log] âŒ è®¡ç®—ä»£è¡¨å®½åº¦å¤±è´¥ %s: %s", archive_path, str(e))
        return 0

def extract_width_from_filename(filename: str) -> int:
    """ä»æ–‡ä»¶åä¸­æå–å®½åº¦ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›0"""
    # åŒ¹é…[æ•°å­—px]æ ¼å¼
    match = re.search(r'\[(\d+)px\]', filename)
    if match:
        try:
            return int(match.group(1))
        except ValueError:
            return 0
    return 0

def safe_move_file(src_path: str, dst_path: str, max_retries: int = 3, delay: float = 1.0) -> bool:
    """
    å®‰å…¨åœ°ç§»åŠ¨æ–‡ä»¶ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶å’Œå®Œæ•´æ€§æ£€æŸ¥
    
    Args:
        src_path: æºæ–‡ä»¶è·¯å¾„
        dst_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        handler: æ—¥å¿—å¤„ç†å™¨
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        delay: é‡è¯•å»¶è¿Ÿæ—¶é—´(ç§’)
        
    Returns:
        bool: ç§»åŠ¨æ˜¯å¦æˆåŠŸ
    """
    import time
    import os
    
    # ç¡®ä¿æºæ–‡ä»¶å­˜åœ¨
    if not os.path.exists(src_path):
        logger.info("[#error_log] âŒ æºæ–‡ä»¶ä¸å­˜åœ¨: %s", src_path)
        return False
        
    # ç¡®ä¿æºæ–‡ä»¶å¯è¯»
    if not os.access(src_path, os.R_OK):
        logger.info("[#error_log] âŒ æºæ–‡ä»¶æ— æ³•è¯»å–: %s", src_path)
        return False
        
    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
    dst_dir = os.path.dirname(dst_path)
    try:
        os.makedirs(dst_dir, exist_ok=True)
    except Exception as e:
        logger.info("[#error_log] âŒ åˆ›å»ºç›®æ ‡ç›®å½•å¤±è´¥: %s, é”™è¯¯: %s", dst_dir, str(e))
        return False
        
    # æ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å¯å†™
    if not os.access(dst_dir, os.W_OK):
        logger.info("[#error_log] âŒ ç›®æ ‡ç›®å½•æ— å†™å…¥æƒé™: %s", dst_dir)
        return False
        
    # è·å–æºæ–‡ä»¶å¤§å°
    try:
        src_size = os.path.getsize(src_path)
    except Exception as e:
        logger.info("[#error_log] âŒ æ— æ³•è·å–æºæ–‡ä»¶å¤§å°: %s, é”™è¯¯: %s", src_path, str(e))
        return False
        
    # é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        try:
            # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆå°è¯•åˆ é™¤
            if os.path.exists(dst_path):
                try:
                    os.remove(dst_path)
                except Exception as e:
                    # è®°å½•é”™è¯¯ä½†ç»§ç»­å°è¯•ç§»åŠ¨
                    logger.info("[#error_log] âš ï¸ æ— æ³•åˆ é™¤å·²å­˜åœ¨çš„ç›®æ ‡æ–‡ä»¶: %s, é”™è¯¯: %s", dst_path, str(e))
                    # å°è¯•ä½¿ç”¨å…¶ä»–æ–¹å¼å¤„ç†
                    try:
                        # 1. å°è¯•ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å
                        temp_dst_path = dst_path + f".temp_{attempt}"
                        shutil.move(src_path, temp_dst_path)
                        # å¦‚æœç§»åŠ¨åˆ°ä¸´æ—¶æ–‡ä»¶æˆåŠŸï¼Œå†å°è¯•é‡å‘½å
                        os.replace(temp_dst_path, dst_path)
                        return True
                    except Exception as move_error:
                        logger.info("[#error_log] âš ï¸ ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç§»åŠ¨å¤±è´¥: %s", str(move_error))
                        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­é‡è¯•
                        if attempt < max_retries - 1:
                            time.sleep(delay)
                            continue
                        return False
            
            # æ‰§è¡Œç§»åŠ¨æ“ä½œ
            shutil.move(src_path, dst_path)
            
            # éªŒè¯ç§»åŠ¨åçš„æ–‡ä»¶
            if not os.path.exists(dst_path):
                raise Exception("ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨")
                
            # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦ä¸€è‡´
            dst_size = os.path.getsize(dst_path)
            if dst_size != src_size:
                raise Exception(f"æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æºæ–‡ä»¶ {src_size} å­—èŠ‚, ç›®æ ‡æ–‡ä»¶ {dst_size} å­—èŠ‚")
                
            return True
            
        except Exception as e:
            if attempt < max_retries - 1:
                logger.info("[#error_log] âš ï¸ ç§»åŠ¨æ–‡ä»¶å¤±è´¥ï¼Œå°è¯•é‡è¯• (%d/%d): %s", attempt + 1, max_retries, str(e))
                time.sleep(delay)
                continue
            else:
                logger.info("[#error_log] âŒ ç§»åŠ¨æ–‡ä»¶å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: %s", str(e))
                # å¦‚æœæœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œæ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦è¿˜å­˜åœ¨
                if not os.path.exists(src_path):
                    logger.info("[#error_log] âŒ æºæ–‡ä»¶å·²ä¸å­˜åœ¨: %s", src_path)
                return False
                
    return False

def process_file_with_count(file_path: str) -> Tuple[str, str, Dict[str, Union[int, float]]]:
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œè¿”å›åŸå§‹è·¯å¾„ã€æ–°è·¯å¾„å’Œæ‰€æœ‰æŒ‡æ ‡"""
    full_path = file_path
    dir_name = os.path.dirname(file_path)
    file_name = os.path.basename(file_path)
    name, ext = os.path.splitext(file_name)
    
    # ç§»é™¤å·²æœ‰çš„æ ‡è®°
    name = re.sub(r'\{[^}]*\}', '', name)  # ç§»é™¤æ‰€æœ‰èŠ±æ‹¬å·å†…å®¹
    
    # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    metrics = {
        'width': 0,
        'page_count': 0,
        'clarity_score': 0.0
    }
    
    # é¦–å…ˆå°è¯•ä»åŸæ–‡ä»¶åæå–é¡µæ•°ä¿¡æ¯
    page_match = re.search(r'\{(\d+)@PX\}', file_name)
    if page_match:
        metrics['page_count'] = int(page_match.group(1))
    else:
        # å¦‚æœæ–‡ä»¶åä¸­æ²¡æœ‰é¡µæ•°ä¿¡æ¯ï¼Œæ‰è®¡ç®—é¡µæ•°
        metrics['page_count'] = get_image_count(full_path)
    
    # è®¡ç®—å®½åº¦
    metrics['width'] = calculate_representative_width(full_path)
    
    # è®¡ç®—æ¸…æ™°åº¦è¯„åˆ†
    try:
        with zipfile.ZipFile(full_path, 'r') as zf:
            image_files = [f for f in zf.namelist() if os.path.splitext(f.lower())[1] in IMAGE_EXTENSIONS]
            if image_files:
                sample_files = random.sample(image_files, min(5, len(image_files)))
                scores = []
                for sample in sample_files:
                    with zf.open(sample) as f:
                        img_data = f.read()
                        scores.append(ImageClarityEvaluator.calculate_definition(img_data))
                metrics['clarity_score'] = sum(scores) / len(scores) if scores else 0.0
                
    except Exception as e:
        logger.error("[#error_log] æ¸…æ™°åº¦è®¡ç®—å¤±è´¥ %s: %s", file_path, str(e))
    
    # ç”Ÿæˆå±æ€§å­—ç¬¦ä¸²ï¼Œæ‰€æœ‰å±æ€§æ”¾åœ¨ä¸€ä¸ªå¤§æ‹¬å·å†…
    parts = []
    if metrics['width'] > 0:
        parts.append(f"{shorten_number_cn(metrics['width'], use_w=True)}@WD")
    if metrics['page_count'] > 0:
        parts.append(f"{shorten_number_cn(metrics['page_count'], use_w=True)}@PX")
    if metrics['clarity_score'] > 0:
        parts.append(f"{shorten_number_cn(int(metrics['clarity_score']), use_w=True)}@DE")
    
    metrics_str = "{" + ",".join(parts) + "}" if parts else ""
    new_name = f"{name}{metrics_str}{ext}"
    new_path = os.path.join(dir_name, new_name) if dir_name else new_name
    
    return file_path, new_path, metrics

def process_file_group(group_files: List[str], base_dir: str, trash_dir: str, report_generator: ReportGenerator, create_shortcuts: bool = False, enable_multi_main: bool = False) -> None:
    """å¤„ç†ä¸€ç»„ç›¸ä¼¼æ–‡ä»¶"""
    # è·å–ç»„çš„åŸºç¡€åç§°
    group_base_name, _ = clean_filename(group_files[0])
    
    # ç”Ÿæˆç»„IDï¼ˆä½¿ç”¨ç»„åçš„å“ˆå¸Œå€¼å4ä½ä½œä¸ºç»„IDï¼‰
    group_id = abs(hash(group_base_name)) % 10000
    
    # è¿‡æ»¤æ‰é»‘åå•æ–‡ä»¶
    filtered_files = [f for f in group_files if not is_in_blacklist(f)]
    if not filtered_files:
        logger.info("[#group_info] â­ï¸ ç»„[%s]è·³è¿‡: æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨é»‘åå•ä¸­", group_base_name)
        return
        
    # åˆ†ç¦»æ±‰åŒ–ç‰ˆæœ¬å’Œå…¶ä»–ç‰ˆæœ¬ï¼Œå¹¶ä½¿ç”¨å®Œæ•´è·¯å¾„
    chinese_versions = []
    other_versions = []
    for f in filtered_files:
        full_path = os.path.join(base_dir, f)
        if is_chinese_version(f):
            chinese_versions.append(full_path)
        else:
            other_versions.append(full_path)
    
    # æ£€æŸ¥æ±‰åŒ–ç‰ˆæœ¬ä¸­æ˜¯å¦æœ‰åŒ…å«åŸç‰ˆå…³é”®è¯çš„
    chinese_has_original = any(has_original_keywords(f) for f in chinese_versions)
    
    # å¦‚æœæ±‰åŒ–ç‰ˆæœ¬ä¸­æ²¡æœ‰åŸç‰ˆå…³é”®è¯ï¼Œåˆ™å°†å…¶ä»–ç‰ˆæœ¬ä¸­åŒ…å«åŸç‰ˆå…³é”®è¯çš„ä¹Ÿå½’ä¸ºéœ€è¦ä¿ç•™çš„ç‰ˆæœ¬
    if not chinese_has_original:
        original_keyword_versions = [f for f in other_versions if has_original_keywords(os.path.basename(f))]
        if original_keyword_versions:
            chinese_versions.extend(original_keyword_versions)
            other_versions = [f for f in other_versions if not has_original_keywords(os.path.basename(f))]
            logger.info("[#file_ops] ğŸ“ å°†%dä¸ªåŒ…å«åŸç‰ˆå…³é”®è¯çš„æ–‡ä»¶å½’å…¥ä¿ç•™åˆ—è¡¨", len(original_keyword_versions))
    
    # ä¸ºæ¯ä¸ªæ–‡ä»¶æ·»åŠ å›¾ç‰‡æ•°é‡æ ‡è®°å’Œè®¡ç®—å®½åº¦
    processed_files = []
    file_metrics = {}  # å­˜å‚¨æ¯ä¸ªæ–‡ä»¶çš„æŒ‡æ ‡
    
    # ç¬¬ä¸€è½®ï¼šè®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„æŒ‡æ ‡ï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´è·¯å¾„
    for file in chinese_versions + other_versions:
        old_path, new_path, metrics = process_file_with_count(file)  # ç°åœ¨ä¼ å…¥çš„æ˜¯å®Œæ•´è·¯å¾„
        processed_files.append((old_path, new_path))
        file_metrics[old_path] = metrics
    
    # æ‰¾å‡ºæœ€ä¼˜æŒ‡æ ‡
    best_metrics = {
        'width': max((m['width'] for m in file_metrics.values()), default=0),
        'page_count': min((m['page_count'] for m in file_metrics.values() if m['page_count'] > 0), default=0),
        'clarity_score': max((m['clarity_score'] for m in file_metrics.values()), default=0)
    }
    
    # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦ç»Ÿä¸€
    metrics_same = {
        'width': len(set(m['width'] for m in file_metrics.values() if m['width'] > 0)) <= 1,
        'page_count': len(set(m['page_count'] for m in file_metrics.values() if m['page_count'] > 0)) <= 1,
        'clarity_score': len(set(m['clarity_score'] for m in file_metrics.values() if m['clarity_score'] > 0)) <= 1
    }
    
    # ç¬¬äºŒè½®ï¼šé‡å‘½åæ–‡ä»¶ï¼Œæ·»åŠ å¸¦emojiçš„æŒ‡æ ‡
    updated_files = []
    for old_path, _ in processed_files:
        metrics = file_metrics[old_path]
        parts = []
        
        # æ·»åŠ ç»„ID
        parts.append(f"ğŸª†G{group_id:04d}")
        
        # æ·»åŠ å®½åº¦ï¼ˆå¦‚æœä¸æ˜¯ç»Ÿä¸€å€¼ä¸”æ˜¯æœ€ä¼˜å€¼åˆ™æ·»åŠ è¡¨æƒ…ï¼‰
        if metrics['width'] > 0:
            width_str = f"{shorten_number_cn(metrics['width'], use_w=True)}@WD"
            if not metrics_same['width'] and metrics['width'] == best_metrics['width']:
                width_str = f"ğŸ“{width_str}"
            parts.append(width_str)
        
        # æ·»åŠ é¡µæ•°ï¼ˆå¦‚æœä¸æ˜¯ç»Ÿä¸€å€¼ä¸”æ˜¯æœ€ä¼˜å€¼åˆ™æ·»åŠ è¡¨æƒ…ï¼‰
        if metrics['page_count'] > 0:
            page_str = f"{shorten_number_cn(metrics['page_count'], use_w=True)}@PX"
            if not metrics_same['page_count'] and metrics['page_count'] == best_metrics['page_count']:
                page_str = f"ğŸ“„{page_str}"
            parts.append(page_str)
        
        # æ·»åŠ æ¸…æ™°åº¦ï¼ˆå¦‚æœä¸æ˜¯ç»Ÿä¸€å€¼ä¸”æ˜¯æœ€ä¼˜å€¼åˆ™æ·»åŠ è¡¨æƒ…ï¼‰
        if metrics['clarity_score'] > 0:
            clarity_str = f"{shorten_number_cn(int(metrics['clarity_score']), use_w=True)}@DE"
            if not metrics_same['clarity_score'] and metrics['clarity_score'] == best_metrics['clarity_score']:
                clarity_str = f"ğŸ”{clarity_str}"
            parts.append(clarity_str)
        
        # æ„å»ºæ–°æ–‡ä»¶å
        dir_name = os.path.dirname(old_path)
        file_name = os.path.basename(old_path)
        name, ext = os.path.splitext(file_name)
        name = re.sub(r'\{[^}]*\}', '', name)  # ç§»é™¤å·²æœ‰çš„æŒ‡æ ‡
        metrics_str = "{" + ",".join(parts) + "}" if parts else ""
        new_name = f"{metrics_str}{name}{ext}"
        new_path = os.path.join(dir_name, new_name)
        
        # é‡å‘½åæ–‡ä»¶
        old_full_path = os.path.join(base_dir, old_path)
        new_full_path = os.path.join(base_dir, new_path)
        try:
            os.rename(old_full_path, new_full_path)
            updated_files.append((old_path, new_path))
            logger.info("[#file_ops] âœ… å·²é‡å‘½å: %s -> %s", old_path, new_path)
        except Exception as e:
            logger.error("[#error_log] âŒ é‡å‘½åå¤±è´¥ %s: %s", old_path, str(e))
            updated_files.append((old_path, old_path))
    
    # æ›´æ–°æ–‡ä»¶è·¯å¾„
    chinese_versions = [new_path for old_path, new_path in updated_files if old_path in chinese_versions]
    other_versions = [new_path for old_path, new_path in updated_files if old_path in other_versions]
    
    # å¤„ç†æ–‡ä»¶ç§»åŠ¨é€»è¾‘
    if chinese_versions:
        # æœ‰æ±‰åŒ–ç‰ˆæœ¬çš„æƒ…å†µ
        if len(chinese_versions) > 1:
            # å¤šä¸ªæ±‰åŒ–ç‰ˆæœ¬ï¼Œç§»åŠ¨åˆ°multi
            multi_dir = os.path.join(base_dir, 'multi')
            os.makedirs(multi_dir, exist_ok=True)
            
            # å¦‚æœå¯ç”¨äº†multi-mainåŠŸèƒ½ï¼Œæ‰¾åˆ°æœ€å¤§çš„æ–‡ä»¶ä½œä¸ºä¸»æ–‡ä»¶
            if enable_multi_main:
                main_file = max(chinese_versions, key=lambda x: os.path.getsize(os.path.join(base_dir, x)))
                if handle_multi_main_file(main_file, base_dir):
                    logger.info("[#file_ops] âœ… å·²å¤„ç†multi-mainæ–‡ä»¶: %s", main_file)
            
            # ç§»åŠ¨æ‰€æœ‰æ–‡ä»¶åˆ°multiç›®å½•
            for file in chinese_versions:
                src_path = os.path.join(base_dir, file)
                rel_path = os.path.relpath(src_path, base_dir)
                dst_path = os.path.join(multi_dir, rel_path)
                if safe_move_file(src_path, dst_path):
                    logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°multi: %s", file)
                    report_generator.update_stats('moved_to_multi')
            
            # ç§»åŠ¨å…¶ä»–éåŸç‰ˆåˆ°trash
            for other_file in other_versions:
                src_path = os.path.join(base_dir, other_file)
                rel_path = os.path.relpath(src_path, base_dir)
                dst_path = os.path.join(trash_dir, rel_path)
                if create_shortcuts:
                    shortcut_path = os.path.splitext(dst_path)[0]
                    if create_shortcut(src_path, shortcut_path):
                        logger.info("[#file_ops] âœ… å·²åˆ›å»ºå¿«æ·æ–¹å¼: %s", other_file)
                        report_generator.update_stats('created_shortcuts')
                else:
                    if safe_move_file(src_path, dst_path):
                        logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°trash: %s", other_file)
                        report_generator.update_stats('moved_to_trash')
        else:
            # åªæœ‰ä¸€ä¸ªéœ€è¦ä¿ç•™çš„ç‰ˆæœ¬
            logger.info("[#group_info] ğŸ” ç»„[%s]å¤„ç†: å‘ç°1ä¸ªéœ€è¦ä¿ç•™çš„ç‰ˆæœ¬ï¼Œä¿æŒåŸä½ç½®", group_base_name)
            # ç§»åŠ¨å…¶ä»–ç‰ˆæœ¬åˆ°trash
            for other_file in other_versions:
                src_path = os.path.join(base_dir, other_file)
                rel_path = os.path.relpath(src_path, base_dir)
                dst_path = os.path.join(trash_dir, rel_path)
                if create_shortcuts:
                    shortcut_path = os.path.splitext(dst_path)[0]
                    if create_shortcut(src_path, shortcut_path):
                        logger.info("[#file_ops] âœ… å·²åˆ›å»ºå¿«æ·æ–¹å¼: %s", other_file)
                        report_generator.update_stats('created_shortcuts')
                else:
                    if safe_move_file(src_path, dst_path):
                        logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°trash: %s", other_file)
                        report_generator.update_stats('moved_to_trash')
    else:
        # æ²¡æœ‰æ±‰åŒ–ç‰ˆæœ¬çš„æƒ…å†µ
        if len(other_versions) > 1:
            # å¤šä¸ªåŸç‰ˆï¼Œç§»åŠ¨åˆ°multi
            multi_dir = os.path.join(base_dir, 'multi')
            os.makedirs(multi_dir, exist_ok=True)
            
            # å¦‚æœå¯ç”¨äº†multi-mainåŠŸèƒ½ï¼Œæ‰¾åˆ°æœ€å¤§çš„æ–‡ä»¶ä½œä¸ºä¸»æ–‡ä»¶
            if enable_multi_main:
                main_file = max(other_versions, key=lambda x: os.path.getsize(os.path.join(base_dir, x)))
                # åˆ›å»ºä¸»æ–‡ä»¶çš„å‰¯æœ¬
                if handle_multi_main_file(main_file, base_dir):
                    logger.info("[#file_ops] âœ… å·²å¤„ç†multi-mainæ–‡ä»¶: %s", main_file)
            
            # ç§»åŠ¨æ‰€æœ‰æ–‡ä»¶åˆ°multiç›®å½•
            for file in other_versions:
                src_path = os.path.join(base_dir, file)
                rel_path = os.path.relpath(src_path, base_dir)
                dst_path = os.path.join(multi_dir, rel_path)
                if safe_move_file(src_path, dst_path):
                    logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°multi: %s", file)
                    report_generator.update_stats('moved_to_multi')
            logger.info("[#group_info] ğŸ” ç»„[%s]å¤„ç†: æœªå‘ç°æ±‰åŒ–ç‰ˆæœ¬ï¼Œå‘ç°%dä¸ªåŸç‰ˆï¼Œå·²ç§»åŠ¨åˆ°multi", group_base_name, len(other_versions))
        else:
            # å•ä¸ªåŸç‰ˆï¼Œä¿æŒåŸä½ç½®
            logger.info("[#group_info] ğŸ” ç»„[%s]å¤„ç†: æœªå‘ç°æ±‰åŒ–ç‰ˆæœ¬ï¼Œä»…æœ‰1ä¸ªåŸç‰ˆï¼Œä¿æŒåŸä½ç½®")

def process_directory(directory: str, report_generator: ReportGenerator, dry_run: bool = False, create_shortcuts: bool = False, enable_multi_main: bool = False) -> None:
    """å¤„ç†å•ä¸ªç›®å½•"""
    # åˆ›å»ºtrashç›®å½•
    trash_dir = os.path.join(directory, 'trash')
    if not dry_run:
        os.makedirs(trash_dir, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰å‹ç¼©æ–‡ä»¶
    all_files = []
    logger.info("[#process] ğŸ” æ­£åœ¨æ‰«ææ–‡ä»¶...")
    
    for root, _, files in os.walk(directory):
        # è·³è¿‡trashå’Œmultiç›®å½•
        if 'trash' in root or 'multi' in root:
            logger.info("[#file_ops] â­ï¸ è·³è¿‡ç›®å½•: %s", root)
            continue
            
        for file in files:
            # ä½¿ç”¨æ–°å®šä¹‰çš„ARCHIVE_EXTENSIONS
            if os.path.splitext(file.lower())[1] in ARCHIVE_EXTENSIONS:
                rel_path = os.path.relpath(os.path.join(root, file), directory)
                all_files.append(rel_path)
                # æ›´æ–°æ‰«æè¿›åº¦
                logger.info("[@process] æ‰«æè¿›åº¦: %d/%d", len(all_files), len(all_files))
    
    if not all_files:
        logger.info("[#error_log] âš ï¸ ç›®å½• %s ä¸­æœªæ‰¾åˆ°å‹ç¼©æ–‡ä»¶", directory)
        return
        
    # æ›´æ–°æŠ¥å‘Šç»Ÿè®¡
    report_generator.update_stats('total_files', len(all_files))
    
    # å¯¹æ–‡ä»¶è¿›è¡Œåˆ†ç»„
    groups = group_similar_files(all_files)
    logger.info("[#stats] ğŸ“Š æ€»è®¡: %dä¸ªæ–‡ä»¶, %dä¸ªç»„", len(all_files), len(groups))
    
    # æ›´æ–°æŠ¥å‘Šç»Ÿè®¡
    report_generator.update_stats('total_groups', len(groups))
    
    # åˆ›å»ºè¿›ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†
    logger.info("[#process] ğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶ç»„...")
    
    with ThreadPoolExecutor(max_workers=min(os.cpu_count() * 2, 8)) as executor:
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        futures = []
        for _, group_files in groups.items():
            if len(group_files) > 1:  # åªå¤„ç†æœ‰å¤šä¸ªç‰ˆæœ¬çš„ç»„
                future = executor.submit(
                    process_file_group,
                    group_files,
                    directory,
                    trash_dir,
                    report_generator,
                    create_shortcuts,
                    enable_multi_main
                )
                futures.append(future)
        
        # æ›´æ–°ç»„å¤„ç†è¿›åº¦
        completed = 0
        for _ in as_completed(futures):
            completed += 1
            future_count = len(futures)
            scan_percent = completed / future_count * 100
            
            logger.info("[@stats] ç»„è¿›åº¦: (%d/%d) %.2f%%", completed, future_count, scan_percent)

def get_paths_from_clipboard():
    """ä»å‰ªè´´æ¿è¯»å–å¤šè¡Œè·¯å¾„"""
    try:
        clipboard_content = pyperclip.paste()
        if not clipboard_content:
            return []
            
        # åˆ†å‰²å¤šè¡Œå†…å®¹å¹¶æ¸…ç†
        paths = [
            path.strip().strip('"').strip("'")
            for path in clipboard_content.splitlines() 
            if path.strip()
        ]
        
        # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨
        valid_paths = [
            path for path in paths 
            if os.path.exists(path)
        ]
        
        if valid_paths:
            logger.info("[#file_ops] ğŸ“‹ ä»å‰ªè´´æ¿è¯»å–åˆ° %d ä¸ªæœ‰æ•ˆè·¯å¾„", len(valid_paths))
        else:
            logger.info("[#error_log] âš ï¸ å‰ªè´´æ¿ä¸­æ²¡æœ‰æœ‰æ•ˆè·¯å¾„")
            
        return valid_paths
        
    except Exception as e:
        logger.info("[#error_log] âŒ è¯»å–å‰ªè´´æ¿æ—¶å‡ºé”™: %s", e)
        return []

def get_long_path_name(path_str: str) -> str:
    """è½¬æ¢ä¸ºé•¿è·¯å¾„æ ¼å¼"""
    if not path_str.startswith("\\\\?\\"):
        if os.path.isabs(path_str):
            return "\\\\?\\" + path_str
    return path_str

def safe_path(path: str) -> str:
    """ç¡®ä¿è·¯å¾„æ”¯æŒé•¿æ–‡ä»¶å"""
    try:
        abs_path = os.path.abspath(path)
        return get_long_path_name(abs_path)
    except Exception as e:
        # add_error_log(f"âŒ å¤„ç†è·¯å¾„æ—¶å‡ºé”™ {path}: {e}")
        return path

def process_paths(paths: List[str]) -> List[str]:
    """å¤„ç†è¾“å…¥çš„è·¯å¾„åˆ—è¡¨"""
    TextualLoggerManager.set_layout(TEXTUAL_LAYOUT,config_info['log_file'])

    # è¿‡æ»¤æ‰ç©ºè·¯å¾„å’Œå¼•å·
    valid_paths = []
    for path in paths:
        # ç§»é™¤è·¯å¾„ä¸¤ç«¯çš„å¼•å·å’Œç©ºç™½å­—ç¬¦
        path = path.strip()
        if path.startswith('"') and path.endswith('"'):
            path = path[1:-1]
        elif path.startswith("'") and path.endswith("'"):
            path = path[1:-1]
        
        if path:
            # å°è¯•è½¬æ¢è·¯å¾„ç¼–ç 
            try:
                # ä½¿ç”¨å®‰å…¨çš„è·¯å¾„å¤„ç†
                safe_path_str = safe_path(path)
                if os.path.exists(safe_path_str):
                    valid_paths.append(safe_path_str)
                else:
                    logger.info("[#error_log] âŒ è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®: %s", path)
            except Exception as e:
                logger.info("[#error_log] âŒ å¤„ç†è·¯å¾„æ—¶å‡ºé”™: %s, é”™è¯¯: %s", path, str(e))
    
    if not valid_paths:
        logger.info("[#error_log] âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„è·¯å¾„")
        
    return valid_paths

def create_shortcut(src_path: str, dst_path: str) -> bool:
    """åˆ›å»ºæŒ‡å‘æºæ–‡ä»¶çš„å¿«æ·æ–¹å¼"""
    try:
        shell = win32com.client.Dispatch("WScript.Shell")
        shortcut = shell.CreateShortCut(dst_path + ".lnk")
        shortcut.Targetpath = src_path
        shortcut.save()
        return True
    except Exception as e:
        logger.error("[#error_log] åˆ›å»ºå¿«æ·æ–¹å¼å¤±è´¥: %s", str(e))
        return False

def handle_multi_main_file(file_path: str, base_dir: str) -> Optional[str]:
    """
    å¤„ç†multiä¸»æ–‡ä»¶ï¼šåˆ›å»ºå‰¯æœ¬å¹¶æ·»åŠ [multi-main]æ ‡è®°
    
    Args:
        file_path: æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
        base_dir: åŸºç¡€ç›®å½•
        
    Returns:
        Optional[str]: æˆåŠŸæ—¶è¿”å›æ–°æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # æ„å»ºå®Œæ•´è·¯å¾„
        full_path = os.path.join(base_dir, file_path)
        dir_name = os.path.dirname(file_path)
        file_name = os.path.basename(file_path)
        name, ext = os.path.splitext(file_name)
        
        # åˆ›å»ºæ–°æ–‡ä»¶åï¼ˆæ·»åŠ [multi-main]æ ‡è®°ï¼‰
        new_name = f"{name}[multi-main]{ext}"
        new_rel_path = os.path.join(dir_name, new_name) if dir_name else new_name
        new_full_path = os.path.join(base_dir, new_rel_path)
        
        # å¤åˆ¶æ–‡ä»¶
        shutil.copy2(full_path, new_full_path)
        logger.info("[#file_ops] âœ… å·²åˆ›å»ºmulti-mainå‰¯æœ¬: %s", new_rel_path)
        return new_rel_path
    except Exception as e:
        logger.error("[#error_log] âŒ åˆ›å»ºmulti-mainå‰¯æœ¬å¤±è´¥ %s: %s", file_path, str(e))
        return None

def setup_cli_parser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='å¤„ç†é‡å¤å‹ç¼©åŒ…æ–‡ä»¶')
    group = parser.add_mutually_exclusive_group()
    group.add_argument('-c', '--clipboard', action='store_true', help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
    group.add_argument('-p', '--paths', nargs='+', help='è¦å¤„ç†çš„ç›®å½•è·¯å¾„')
    parser.add_argument('-s', '--sample-count', type=int, default=3, help='æ¯ä¸ªå‹ç¼©åŒ…æŠ½å–çš„å›¾ç‰‡æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤3ï¼‰')
    parser.add_argument('--create-shortcuts', action='store_true', help='åˆ›å»ºå¿«æ·æ–¹å¼è€Œä¸æ˜¯ç§»åŠ¨æ–‡ä»¶')
    parser.add_argument('--enable-multi-main', action='store_true', help='ä¸ºæ¯ä¸ªmultiç»„åˆ›å»ºä¸»æ–‡ä»¶å‰¯æœ¬')
    parser.add_argument('--report', type=str, help='æŒ‡å®šæŠ¥å‘Šæ–‡ä»¶åï¼ˆé»˜è®¤ä¸º"å¤„ç†æŠ¥å‘Š_æ—¶é—´æˆ³.md"ï¼‰')
    return parser

def run_application(args):
    """è¿è¡Œåº”ç”¨ç¨‹åºçš„ä¸»å‡½æ•°"""
    # è·å–è¦å¤„ç†çš„è·¯å¾„
    paths = []
    
    # ä»å‰ªè´´æ¿è¯»å–
    if hasattr(args, 'clipboard') and args.clipboard:
        paths.extend(get_paths_from_clipboard())
    # ä»å‘½ä»¤è¡Œå‚æ•°è¯»å–
    elif hasattr(args, 'paths') and args.paths:
        if isinstance(args.paths, str):
            # å¦‚æœæ˜¯TUIæ¨¡å¼ä¼ å…¥çš„å­—ç¬¦ä¸²ï¼ŒæŒ‰é€—å·åˆ†å‰²
            paths.extend([p.strip() for p in args.paths.split(',') if p.strip()])
        else:
            # å‘½ä»¤è¡Œæ¨¡å¼ä¼ å…¥çš„åˆ—è¡¨
            paths.extend(args.paths)
    # é»˜è®¤ä»ç»ˆç«¯è¾“å…¥
    else:
        print("è¯·è¾“å…¥è¦å¤„ç†çš„è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
        while True:
            try:
                line = input().strip()
                if not line:  # ç©ºè¡Œç»“æŸè¾“å…¥
                    break
                paths.append(line)
            except EOFError:
                break
            except KeyboardInterrupt:
                print("ç”¨æˆ·å–æ¶ˆè¾“å…¥")
                return False
        
    if not paths:
        logger.info("[#error_log] âŒ æœªæä¾›ä»»ä½•è·¯å¾„")
        return False
        
    # å¤„ç†å’ŒéªŒè¯æ‰€æœ‰è·¯å¾„
    valid_paths = process_paths(paths)
    
    if not valid_paths:
        logger.info("[#error_log] âŒ æ²¡æœ‰æœ‰æ•ˆçš„è·¯å¾„å¯å¤„ç†")
        return False
    
    # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
    report_generator = ReportGenerator()
    
    # å¤„ç†æ¯ä¸ªè·¯å¾„
    for path in valid_paths:
        logger.info("[#process] ğŸš€ å¼€å§‹å¤„ç†ç›®å½•: %s", path)
        process_directory(
            path,
            report_generator,
            create_shortcuts=args.create_shortcuts if hasattr(args, 'create_shortcuts') else False,
            enable_multi_main=args.enable_multi_main if hasattr(args, 'enable_multi_main') else False
        )
        logger.info("[#process] âœ¨ ç›®å½•å¤„ç†å®Œæˆ: %s", path)
        
        # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        if hasattr(args, 'report') and args.report:
            report_path = report_generator.save_report(path, args.report)
        else:
            report_path = report_generator.save_report(path)
            
        if report_path:
            logger.info("[#process] ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: %s", report_path)
        else:
            logger.info("[#error_log] âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    parser = setup_cli_parser()
    
    # åˆ›å»ºé¢„è®¾é…ç½®
    preset_configs = {
        "åŸºæœ¬æ¨¡å¼": {
            "description": "ä»å‰ªè´´æ¿è¯»å–è·¯å¾„ï¼Œæ‰§è¡Œæ ‡å‡†å¤„ç†",
            "checkbox_options": ["--clipboard"],
            "input_values": {}
        },
        "å¿«æ·æ–¹å¼æ¨¡å¼": {
            "description": "åˆ›å»ºå¿«æ·æ–¹å¼è€Œä¸æ˜¯ç§»åŠ¨æ–‡ä»¶",
            "checkbox_options": ["--clipboard", "--create-shortcuts"],
            "input_values": {}
        },
        "å¤šæ–‡ä»¶ä¿ç•™æ¨¡å¼": {
            "description": "ä¸ºæ¯ä¸ªmultiç»„åˆ›å»ºä¸»æ–‡ä»¶å‰¯æœ¬",
            "checkbox_options": ["--clipboard", "--enable-multi-main"],
            "input_values": {}
        },
        "å®Œæ•´æ¨¡å¼": {
            "description": "å¯ç”¨æ‰€æœ‰é«˜çº§åŠŸèƒ½",
            "checkbox_options": ["--clipboard", "--create-shortcuts", "--enable-multi-main"],
            "input_values": {
                "--sample-count": "5"
            }
        }
    }
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    has_args = len(sys.argv) > 1
    
    if has_args:
        # ç›´æ¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¿è¡Œ
        args = parser.parse_args(sys.argv[1:])
        run_application(args)
    else:
        # ä½¿ç”¨rich_preseté…ç½®ç•Œé¢
        from nodes.tui.preset.rich_preset import create_config_app
        
        result = create_config_app(
            program=sys.argv[0],
            title="æ–‡ä»¶å»é‡å·¥å…·é…ç½®",
            parser=parser,
            preset_configs=preset_configs
        )
        
        if result:
            run_application(result.args)
        else:
            print("æ“ä½œå·²å–æ¶ˆ")

if __name__ == "__main__":
    main()