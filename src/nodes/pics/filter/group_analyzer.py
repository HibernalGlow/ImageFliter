"""
æ–‡ä»¶ç»„åˆ†æžå™¨æ¨¡å—
ç”¨äºŽåˆ†æžå’Œæ¯”è¾ƒåŒä¸€ç»„å†…çš„å¤šä¸ªæ–‡ä»¶ï¼Œæå–æœ€ä¼˜æŒ‡æ ‡
"""

import os
import re
import logging
from typing import List, Dict, Tuple, Optional, Set
from dataclasses import dataclass
from nodes.utils.number_shortener import shorten_number_cn

logger = logging.getLogger(__name__)

@dataclass
class FileMetrics:
    """æ–‡ä»¶æŒ‡æ ‡æ•°æ®ç±»"""
    width: int = 0
    page_count: int = 0
    clarity_score: float = 0.0
    
    def __str__(self) -> str:
        parts = []
        if self.width > 0:
            parts.append(f"{self.width}@WD")
        if self.page_count > 0:
            parts.append(f"{self.page_count}@PX")
        if self.clarity_score > 0:
            parts.append(f"{int(self.clarity_score)}@DE")
        return "{" + ",".join(parts) + "}" if parts else ""

class GroupAnalyzer:
    """æ–‡ä»¶ç»„åˆ†æžå™¨ï¼Œç”¨äºŽåˆ†æžåŒä¸€ç»„å†…çš„å¤šä¸ªæ–‡ä»¶å¹¶æå–æœ€ä¼˜æŒ‡æ ‡"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æžå™¨"""
        self.emoji_map = {
            'width': 'ðŸ“',  # å®½åº¦ä½¿ç”¨å°ºå­emoji
            'page_count': 'ðŸ“„',  # é¡µæ•°ä½¿ç”¨çº¸å¼ emoji
            'clarity_score': 'ðŸ”'  # æ¸…æ™°åº¦ä½¿ç”¨æ”¾å¤§é•œemoji
        }
    
    def clean_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œåªä¿ç•™ä¸»æ–‡ä»¶åéƒ¨åˆ†è¿›è¡Œæ¯”è¾ƒ"""
        # ç§»é™¤æ‰©å±•å
        name = os.path.splitext(filename)[0]
        
        # ç§»é™¤æ‰€æœ‰æ‹¬å·å†…å®¹å’ŒæŒ‡æ ‡ä¿¡æ¯
        name = re.sub(r'\[[^\]]*\]|\([^)]*\)|\{[^}]*\}', '', name)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
        name = re.sub(r'[^\w\s\-]', '', name)
        name = re.sub(r'\s+', '', name)
        
        return name.strip().lower()
    
    def group_similar_files(self, files: List[str]) -> Dict[str, List[str]]:
        """å°†ç›¸ä¼¼æ–‡ä»¶åˆ†ç»„
        
        Args:
            files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            Dict[str, List[str]]: ä»¥æ¸…ç†åŽçš„æ–‡ä»¶åä¸ºé”®ï¼ŒåŽŸå§‹æ–‡ä»¶åˆ—è¡¨ä¸ºå€¼çš„å­—å…¸
        """
        groups: Dict[str, List[str]] = {}
        logger.info("ðŸ” å¼€å§‹æ–‡ä»¶åˆ†ç»„...")
        
        for file in files:
            clean_name = self.clean_filename(file)
            if not clean_name:
                logger.info(f"âš ï¸ è·³è¿‡æ— æ•ˆæ–‡ä»¶å: {file}")
                continue
                
            if clean_name not in groups:
                groups[clean_name] = []
            groups[clean_name].append(file)
            logger.debug(f"ðŸ“‘ æ–‡ä»¶ '{file}' è¢«åˆ†ç»„åˆ° '{clean_name}'")
        
        # è®°å½•åˆ†ç»„ç»“æžœ
        for clean_name, group_files in groups.items():
            if len(group_files) > 1:
                logger.info(f"ðŸ“¦ æ‰¾åˆ°ç»„ '{clean_name}': {len(group_files)}ä¸ªæ–‡ä»¶")
                for f in group_files:
                    logger.debug(f"  - {f}")
        
        return groups
    
    def extract_metrics(self, filename: str) -> Optional[FileMetrics]:
        """ä»Žæ–‡ä»¶åä¸­æå–æŒ‡æ ‡ä¿¡æ¯"""
        try:
            metrics = FileMetrics()
            
            # åŒ¹é…èŠ±æ‹¬å·ä¸­çš„æŒ‡æ ‡ä¿¡æ¯
            pattern = r'\{([^}]+)\}'
            match = re.search(pattern, filename)
            if not match:
                return None
                
            metrics_str = match.group(1)
            # åˆ†æžå„ä¸ªæŒ‡æ ‡
            for metric in metrics_str.split(','):
                if '@WD' in metric:
                    width_str = metric.replace('@WD', '').strip()
                    try:
                        metrics.width = int(float(width_str))
                    except ValueError:
                        pass
                elif '@PX' in metric:
                    page_str = metric.replace('@PX', '').strip()
                    try:
                        metrics.page_count = int(float(page_str))
                    except ValueError:
                        pass
                elif '@DE' in metric:
                    clarity_str = metric.replace('@DE', '').strip()
                    try:
                        metrics.clarity_score = float(clarity_str)
                    except ValueError:
                        pass
            
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ æå–æŒ‡æ ‡å¤±è´¥ {filename}: {str(e)}")
            return None
    
    def analyze_group(self, files: List[str]) -> Dict[str, Tuple[float, str]]:
        """åˆ†æžæ–‡ä»¶ç»„ï¼Œè¿”å›žæ¯ä¸ªæŒ‡æ ‡çš„æœ€ä¼˜å€¼å’Œå¯¹åº”æ–‡ä»¶"""
        logger.info(f"ðŸ” å¼€å§‹åˆ†æžæ–‡ä»¶ç»„: {len(files)}ä¸ªæ–‡ä»¶")
        
        best_metrics = {
            'width': (0, ''),
            'page_count': (0, ''),
            'clarity_score': (0, '')
        }
        
        all_metrics = []
        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„æŒ‡æ ‡
        for file in files:
            metrics = self.extract_metrics(file)
            if metrics:
                all_metrics.append((file, metrics))
                logger.info(f"ðŸ“Š æ–‡ä»¶æŒ‡æ ‡: {file} -> {metrics}")
                
                # æ›´æ–°æœ€ä¼˜å€¼
                if metrics.width > best_metrics['width'][0]:
                    best_metrics['width'] = (metrics.width, file)
                if metrics.page_count > best_metrics['page_count'][0]:
                    best_metrics['page_count'] = (metrics.page_count, file)
                if metrics.clarity_score > best_metrics['clarity_score'][0]:
                    best_metrics['clarity_score'] = (metrics.clarity_score, file)
        
        # æ£€æŸ¥å“ªäº›æŒ‡æ ‡æ˜¯ç»Ÿä¸€çš„
        unified_metrics = self._find_unified_metrics(all_metrics)
        logger.info(f"ðŸŽ¯ ç»Ÿä¸€æŒ‡æ ‡: {unified_metrics}")
        
        # è¿”å›žéžç»Ÿä¸€çš„æœ€ä¼˜æŒ‡æ ‡
        return {
            metric: (value, file)
            for metric, (value, file) in best_metrics.items()
            if metric not in unified_metrics
        }
    
    def _find_unified_metrics(self, metrics_list: List[Tuple[str, FileMetrics]]) -> List[str]:
        """æ‰¾å‡ºæ‰€æœ‰æ–‡ä»¶éƒ½ç›¸åŒçš„æŒ‡æ ‡"""
        if not metrics_list:
            return []
            
        unified = []
        first_metrics = metrics_list[0][1]
        
        # æ£€æŸ¥æ¯ä¸ªæŒ‡æ ‡æ˜¯å¦ç»Ÿä¸€
        if all(m[1].width == first_metrics.width for m in metrics_list):
            unified.append('width')
        if all(m[1].page_count == first_metrics.page_count for m in metrics_list):
            unified.append('page_count')
        if all(m[1].clarity_score == first_metrics.clarity_score for m in metrics_list):
            unified.append('clarity_score')
            
        return unified
    
    def format_best_metrics(self, best_metrics: Dict[str, Tuple[float, str]]) -> str:
        """æ ¼å¼åŒ–æœ€ä¼˜æŒ‡æ ‡ä¿¡æ¯"""
        parts = []
        
        # æ·»åŠ æ¯ä¸ªéžç»Ÿä¸€çš„æœ€ä¼˜æŒ‡æ ‡
        for metric, (value, _) in best_metrics.items():
            if value > 0:
                emoji = self.emoji_map.get(metric, '')
                if metric == 'width':
                    parts.append(f"{emoji}{value}@WD")
                elif metric == 'page_count':
                    parts.append(f"{emoji}{int(value)}@PX")
                elif metric == 'clarity_score':
                    parts.append(f"{emoji}{int(value)}@DE")
        
        return "{" + ",".join(parts) + "}" if parts else ""