import os
import logging
from typing import List, Set, Dict, Tuple, Union
from nodes.pics.hash.calculate_hash_custom import ImageHashCalculator, PathURIGenerator
from nodes.pics.filter.watermark_detector import WatermarkDetector
from nodes.pics.filter.cv_text_image_detector import CVTextImageDetector
from PIL import Image
import pillow_avif  # AVIFæ”¯æŒ
import pillow_jxl 
from io import BytesIO
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing
from nodes.hash.hash_accelerator import HashAccelerator
import mmap
import numpy as np
from nodes.record.logger_config import setup_logger

config = {
    'script_name': 'nodes.pics.image_filter',
    'console_enabled': True
}
logger, config_info = setup_logger(config)

class ImageFilter:
    """å›¾ç‰‡è¿‡æ»¤å™¨ï¼Œæ”¯æŒå¤šç§ç‹¬ç«‹çš„è¿‡æ»¤åŠŸèƒ½"""
    
    def __init__(self, hash_file: str = None, hamming_threshold: int = 12, ref_hamming_threshold: int = None, max_workers: int = None):
        """
        åˆå§‹åŒ–è¿‡æ»¤å™¨
        
        Args:
            hash_file: å“ˆå¸Œæ–‡ä»¶è·¯å¾„
            hamming_threshold: æ±‰æ˜è·ç¦»é˜ˆå€¼
            ref_hamming_threshold: å“ˆå¸Œæ–‡ä»¶è¿‡æ»¤çš„æ±‰æ˜è·ç¦»é˜ˆå€¼ï¼Œé»˜è®¤ä½¿ç”¨hamming_threshold
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
        """
        self.hash_file = hash_file
        self.hamming_threshold = hamming_threshold
        self.ref_hamming_threshold = ref_hamming_threshold if ref_hamming_threshold is not None else hamming_threshold
        self.hash_cache = {}  # åˆå§‹åŒ–ç©ºç¼“å­˜
        if hash_file:
            self.hash_cache = self._load_hash_file()
        self.watermark_detector = WatermarkDetector()
        self.text_detector = CVTextImageDetector()
        self.max_workers = max_workers or multiprocessing.cpu_count()
        self.file_cache = {}  # æ·»åŠ æ–‡ä»¶ç¼“å­˜
        self.cache_size_limit = 100 * 1024 * 1024  # 100MBç¼“å­˜é™åˆ¶
        self.current_cache_size = 0
        
    def _load_hash_file(self) -> Dict:
        """åŠ è½½å“ˆå¸Œæ–‡ä»¶"""
        try:
            if not os.path.exists(self.hash_file):
                logger.error(f"å“ˆå¸Œæ–‡ä»¶ä¸å­˜åœ¨: {self.hash_file}")
                return {}
                
            with open(self.hash_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"æˆåŠŸåŠ è½½å“ˆå¸Œæ–‡ä»¶: {self.hash_file}")
            return data.get('hashes', {})
        except Exception as e:
            logger.error(f"åŠ è½½å“ˆå¸Œæ–‡ä»¶å¤±è´¥: {e}")
            return {}

    def _read_file_optimized(self, file_path: str) -> bytes:
        """ä¼˜åŒ–çš„æ–‡ä»¶è¯»å–æ–¹æ³•
        
        ä½¿ç”¨å¤šç§ç­–ç•¥è¯»å–æ–‡ä»¶:
        1. é¦–å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        2. å¯¹äºå°æ–‡ä»¶(<10MB)ä½¿ç”¨æ™®é€šè¯»å–
        3. å¯¹äºå¤§æ–‡ä»¶ä½¿ç”¨mmap
        4. ç»´æŠ¤ç¼“å­˜å¤§å°é™åˆ¶
        """
        try:
            # æ£€æŸ¥ç¼“å­˜
            if file_path in self.file_cache:
                return self.file_cache[file_path]

            file_size = os.path.getsize(file_path)
            
            # å°æ–‡ä»¶ç›´æ¥è¯»å–
            if file_size < 10 * 1024 * 1024:  # 10MB
                with open(file_path, 'rb') as f:
                    data = f.read()
            else:
                # å¤§æ–‡ä»¶ä½¿ç”¨mmap
                with open(file_path, 'rb') as f:
                    with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                        data = mm.read()

            # ç¼“å­˜ç®¡ç†
            if self.current_cache_size + file_size <= self.cache_size_limit:
                self.file_cache[file_path] = data
                self.current_cache_size += file_size
            elif len(self.file_cache) > 0:
                # å¦‚æœç¼“å­˜æ»¡äº†ï¼Œç§»é™¤æœ€æ—©çš„é¡¹ç›®
                oldest_file = next(iter(self.file_cache))
                oldest_size = len(self.file_cache[oldest_file])
                del self.file_cache[oldest_file]
                self.current_cache_size -= oldest_size
                
                # å°è¯•ç¼“å­˜æ–°æ–‡ä»¶
                if self.current_cache_size + file_size <= self.cache_size_limit:
                    self.file_cache[file_path] = data
                    self.current_cache_size += file_size

            return data
            
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def _process_small_images(self, cover_files: List[str], min_size: int) -> Tuple[Set[str], Dict[str, Dict]]:
        """å¤„ç†å°å›¾è¿‡æ»¤"""
        to_delete = set()
        removal_reasons = {}
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_file = {
                executor.submit(self._process_single_image, img_path, min_size): img_path
                for img_path in cover_files
            }
            
            for future in as_completed(future_to_file):
                img_path = future_to_file[future]
                try:
                    result = future.result()
                    if result:
                        should_delete, reason = result
                        if should_delete:
                            to_delete.add(img_path)
                            removal_reasons[img_path] = {
                                'reason': 'small_image',
                                'details': f'å°äº{min_size}åƒç´ '
                            }
                            logger.info(f"æ ‡è®°åˆ é™¤å°å›¾: {os.path.basename(img_path)}")
                except Exception as e:
                    logger.error(f"å¤„ç†å°å›¾æ£€æµ‹å¤±è´¥ {img_path}: {e}")
                
        return to_delete, removal_reasons

    def _process_single_image(self, img_path: str, min_size: int) -> Tuple[bool, str]:
        """å¤„ç†å•ä¸ªå›¾ç‰‡"""
        try:
            # ä½¿ç”¨ä¼˜åŒ–çš„æ–‡ä»¶è¯»å–
            img_data = self._read_file_optimized(img_path)
            if img_data is None:
                return False, None
                
            result, reason = self.detect_small_image(img_data, {'min_size': min_size})
            return reason == 'small_image', reason
            
        except Exception as e:
            logger.error(f"å¤„ç†å›¾ç‰‡å¤±è´¥ {img_path}: {e}")
            return False, None

    def _process_grayscale_images(self, cover_files: List[str]) -> Tuple[Set[str], Dict[str, Dict]]:
        """å¤„ç†é»‘ç™½å›¾è¿‡æ»¤"""
        to_delete = set()
        removal_reasons = {}
        
        for img_path in cover_files:
            try:
                with open(img_path, 'rb') as f:
                    img_data = f.read()
                result, reason = self.detect_grayscale_image(img_data)
                if reason in ['grayscale', 'pure_white', 'pure_black']:
                    to_delete.add(img_path)
                    removal_reasons[img_path] = {
                        'reason': reason,
                        'details': {
                            'grayscale': 'ç°åº¦å›¾ç‰‡',
                            'pure_white': 'çº¯ç™½å›¾ç‰‡',
                            'pure_black': 'çº¯é»‘å›¾ç‰‡'
                        }.get(reason, 'é»‘ç™½å›¾ç‰‡')
                    }
                    logger.info(f"æ ‡è®°åˆ é™¤{removal_reasons[img_path]['details']}: {os.path.basename(img_path)}")
            except Exception as e:
                logger.error(f"å¤„ç†ç°åº¦å›¾æ£€æµ‹å¤±è´¥ {img_path}: {e}")
                
        return to_delete, removal_reasons

    def _process_text_images(self, image_files: List[str], threshold: float = 0.5) -> Tuple[Set[str], Dict[str, Dict]]:
        """å¤„ç†çº¯æ–‡æœ¬å›¾ç‰‡è¿‡æ»¤"""
        to_delete = set()
        removal_reasons = {}
        
        # ä½¿ç”¨å°è£…å¥½çš„æ–¹æ³•è¿›è¡Œè¿‡æ»¤ï¼Œä¸æ°´å°è¿‡æ»¤ç›¸ä¼¼çš„è®¾è®¡
        deleted_files = self._apply_text_filter(image_files, threshold)
        for img, score in deleted_files:
            to_delete.add(img)
            removal_reasons[img] = {
                'reason': 'text_image',
                'details': 'çº¯æ–‡æœ¬å›¾ç‰‡',
                'score': score
            }
            logger.info(f"æ ‡è®°åˆ é™¤çº¯æ–‡æœ¬å›¾ç‰‡: {os.path.basename(img)}")
        
        return to_delete, removal_reasons

    def _apply_text_filter(self, image_files: List[str], threshold: float = 0.5) -> List[Tuple[str, float]]:
        """
        åº”ç”¨çº¯æ–‡æœ¬å›¾ç‰‡è¿‡æ»¤ï¼Œè¿”å›è¦åˆ é™¤çš„å›¾ç‰‡å’Œæ£€æµ‹åˆ†æ•°
        
        Args:
            image_files: å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            threshold: æ–‡æœ¬å›¾ç‰‡æ£€æµ‹é˜ˆå€¼
        
        Returns:
            List[Tuple[str, float]]: (å¾…åˆ é™¤å›¾ç‰‡è·¯å¾„, æ–‡æœ¬æ£€æµ‹åˆ†æ•°)åˆ—è¡¨
        """
        to_delete = []
        
        # æ£€æµ‹æ¯å¼ å›¾ç‰‡æ˜¯å¦ä¸ºæ–‡æœ¬å›¾ç‰‡
        for img_path in image_files:
            try:
                is_text_image, result = self.text_detector.detect_text_image(img_path, threshold)
                score = result.get('total_score', 0) if isinstance(result, dict) else result
                
                if is_text_image:
                    to_delete.append((img_path, score))
                    # åªè®°å½•åŸºç¡€åç§°ï¼Œä¸è®°å½•å®Œæ•´è·¯å¾„
                    logger.info(f"å›¾ç‰‡æ–‡æœ¬æ£€æµ‹ç»“æœ [{os.path.basename(img_path)}]: æ€»åˆ†={score}/4, æ˜¯å¦æ–‡æœ¬å›¾ç‰‡={is_text_image}")
            except Exception as e:
                logger.error(f"å¤„ç†çº¯æ–‡æœ¬å›¾ç‰‡æ£€æµ‹å¤±è´¥ {img_path}: {e}")
        
        return to_delete

    def _process_watermark_images(self, group: List[str], watermark_keywords: List[str] = None) -> Tuple[Set[str], Dict[str, Dict]]:
        """å¤„ç†æ°´å°è¿‡æ»¤"""
        to_delete = set()
        removal_reasons = {}
        
        deleted_files = self._apply_watermark_filter(group, watermark_keywords)
        for img, texts in deleted_files:
            to_delete.add(img)
            removal_reasons[img] = {
                'reason': 'watermark',
                'watermark_texts': texts,
                'matched_keywords': [kw for kw in (watermark_keywords or []) if any(kw in text for text in texts)]
            }
            logger.info(f"æ ‡è®°åˆ é™¤æœ‰æ°´å°å›¾ç‰‡: {os.path.basename(img)}")
            
        return to_delete, removal_reasons
        
    def _process_quality_images(self, group: List[str]) -> Tuple[Set[str], Dict[str, Dict]]:
        """å¤„ç†è´¨é‡è¿‡æ»¤"""
        to_delete = set()
        removal_reasons = {}
        
        deleted_files = self._apply_quality_filter(group)
        for img, size_diff in deleted_files:
            to_delete.add(img)
            removal_reasons[img] = {
                'reason': 'quality',
                'size_diff': size_diff
            }
            logger.info(f"æ ‡è®°åˆ é™¤è¾ƒå°å›¾ç‰‡: {os.path.basename(img)}")
            
        return to_delete, removal_reasons

    def _process_hash_images(self, group: List[str], ref_hamming_threshold: int = None) -> Tuple[Set[str], Dict[str, Dict]]:
        """å¤„ç†å“ˆå¸Œæ–‡ä»¶è¿‡æ»¤"""
        to_delete = set()
        removal_reasons = {}
        
        # ä½¿ç”¨ä¼ å…¥çš„é˜ˆå€¼æˆ–é»˜è®¤å€¼
        threshold = ref_hamming_threshold if ref_hamming_threshold is not None else self.ref_hamming_threshold
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè®¡ç®—å“ˆå¸Œå€¼
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # å¹¶è¡Œè®¡ç®—å“ˆå¸Œå€¼
            future_to_img = {
                executor.submit(self._get_image_hash_and_uri, img_path): img_path 
                for img_path in group
            }
            
            # æ”¶é›†ç»“æœ
            hash_values = {}
            for future in as_completed(future_to_img):
                img_path = future_to_img[future]
                try:
                    result = future.result()
                    if result:
                        uri, hash_value = result
                        hash_values[img_path] = (uri, hash_value)
                except Exception as e:
                    logger.error(f"è®¡ç®—å“ˆå¸Œå€¼å¤±è´¥ {img_path}: {e}")
        
        # è¯»å–å“ˆå¸Œæ–‡ä»¶
        try:
            with open(self.hash_file, 'r', encoding='utf-8') as f:
                hash_data = json.load(f).get('hashes', {})
        except Exception as e:
            logger.error(f"è¯»å–å“ˆå¸Œæ–‡ä»¶å¤±è´¥: {e}")
            return to_delete, removal_reasons
        
        # å¹¶è¡Œæ¯”è¾ƒå“ˆå¸Œå€¼
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # åˆ›å»ºæ¯”è¾ƒä»»åŠ¡
            future_to_comparison = {
                executor.submit(
                    self._compare_hash_with_reference,
                    img_path, current_uri, current_hash,
                    hash_data, threshold
                ): img_path
                for img_path, (current_uri, current_hash) in hash_values.items()
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_comparison):
                img_path = future_to_comparison[future]
                try:
                    result = future.result()
                    if result:
                        match_uri, distance = result
                        to_delete.add(img_path)
                        removal_reasons[img_path] = {
                            'reason': 'hash_duplicate',
                            'ref_uri': match_uri,
                            'distance': distance
                        }
                        logger.info(f"æ ‡è®°åˆ é™¤é‡å¤å›¾ç‰‡: {os.path.basename(img_path)} (å‚è€ƒURI: {match_uri}, è·ç¦»: {distance})")
                except Exception as e:
                    logger.error(f"æ¯”è¾ƒå“ˆå¸Œå€¼å¤±è´¥ {img_path}: {e}")
                    
        return to_delete, removal_reasons

    def _get_image_hash_and_uri(self, img_path: str) -> Tuple[str, str]:
        """è·å–å›¾ç‰‡çš„å“ˆå¸Œå€¼å’ŒURI"""
        try:
            if not os.path.exists(img_path):
                return None
            
            uri = PathURIGenerator.generate(img_path)
            if not uri:
                return None
                
            hash_value = self._get_image_hash(img_path)
            if not hash_value:
                return None
                
            return uri, hash_value
            
        except Exception as e:
            logger.error(f"è·å–å›¾ç‰‡å“ˆå¸Œå’ŒURIå¤±è´¥ {img_path}: {e}")
            return None

    def _compare_hash_with_reference(self, img_path: str, current_uri: str, current_hash: str, 
                                   hash_data: Dict, threshold: int) -> Tuple[str, int]:
        """æ¯”è¾ƒå“ˆå¸Œå€¼ä¸å‚è€ƒå“ˆå¸Œå€¼"""
        try:
            # ä½¿ç”¨åŠ é€Ÿå™¨è¿›è¡Œæ‰¹é‡æ¯”è¾ƒ
            ref_hashes = []
            uri_map = {}
            
            # æ”¶é›†å‚è€ƒå“ˆå¸Œå€¼
            for uri, ref_data in hash_data.items():
                if uri == current_uri:
                    continue
                    
                ref_hash = ref_data.get('hash') if isinstance(ref_data, dict) else str(ref_data)
                if not ref_hash:
                    continue
                    
                ref_hashes.append(ref_hash)
                uri_map[ref_hash] = uri
            
            # ä½¿ç”¨åŠ é€Ÿå™¨æŸ¥æ‰¾ç›¸ä¼¼å“ˆå¸Œ
            similar_hashes = HashAccelerator.find_similar_hashes(
                current_hash,
                ref_hashes,
                uri_map,
                threshold
            )
            
            # å¦‚æœæ‰¾åˆ°ç›¸ä¼¼å“ˆå¸Œ,è¿”å›ç¬¬ä¸€ä¸ª(æœ€ç›¸ä¼¼çš„)
            if similar_hashes:
                ref_hash, uri, distance = similar_hashes[0]
                return uri, distance
                
            return None
            
        except Exception as e:
            logger.error(f"æ¯”è¾ƒå“ˆå¸Œå€¼å¤±è´¥ {img_path}: {e}")
            return None

    def process_images(
        self, 
        image_files: List[str],
        enable_small_filter: bool = None,
        enable_grayscale_filter: bool = None,
        enable_duplicate_filter: bool = None,
        enable_text_filter: bool = None,
        min_size: int = 631,
        duplicate_filter_mode: str = 'quality',  # 'quality', 'watermark' or 'hash'
        watermark_keywords: List[str] = None,  # æ°´å°å…³é”®è¯åˆ—è¡¨
        ref_hamming_threshold: int = None,  # å“ˆå¸Œæ–‡ä»¶è¿‡æ»¤çš„æ±‰æ˜è·ç¦»é˜ˆå€¼
        text_threshold: float = 0.5,  # çº¯æ–‡æœ¬å›¾ç‰‡æ£€æµ‹é˜ˆå€¼
        **kwargs
    ) -> Tuple[Set[str], Dict[str, Dict]]:
        """
        å¤„ç†å›¾ç‰‡åˆ—è¡¨ï¼Œæ”¯æŒå¤šç§ç‹¬ç«‹çš„è¿‡æ»¤åŠŸèƒ½
        
        Args:
            image_files: å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            enable_small_filter: æ˜¯å¦å¯ç”¨å°å›¾è¿‡æ»¤
            enable_grayscale_filter: æ˜¯å¦å¯ç”¨é»‘ç™½å›¾è¿‡æ»¤
            enable_duplicate_filter: æ˜¯å¦å¯ç”¨é‡å¤å›¾ç‰‡è¿‡æ»¤
            enable_text_filter: æ˜¯å¦å¯ç”¨çº¯æ–‡æœ¬å›¾ç‰‡è¿‡æ»¤
            min_size: æœ€å°å›¾ç‰‡å°ºå¯¸
            duplicate_filter_mode: é‡å¤å›¾ç‰‡è¿‡æ»¤æ¨¡å¼ ('quality', 'watermark' æˆ– 'hash')
            watermark_keywords: æ°´å°å…³é”®è¯åˆ—è¡¨ï¼ŒNoneæ—¶ä½¿ç”¨é»˜è®¤åˆ—è¡¨
            ref_hamming_threshold: å“ˆå¸Œæ–‡ä»¶è¿‡æ»¤çš„æ±‰æ˜è·ç¦»é˜ˆå€¼ï¼ŒNoneæ—¶ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„å€¼
            text_threshold: çº¯æ–‡æœ¬å›¾ç‰‡æ£€æµ‹é˜ˆå€¼
            **kwargs: å…¶ä»–å¯æ‰©å±•çš„å‚æ•°
            
        Returns:
            Tuple[Set[str], Dict[str, Dict]]: (è¦åˆ é™¤çš„æ–‡ä»¶é›†åˆ, åˆ é™¤åŸå› å­—å…¸)
        """
        sorted_files = sorted(image_files)
        
        if not sorted_files:
            return set(), {}
            
        logger.info(f"å¼€å§‹å¤„ç†{len(sorted_files)}å¼ å›¾ç‰‡")
        
        to_delete = set()
        removal_reasons = {}
        
        # 1. å°å›¾è¿‡æ»¤
        if enable_small_filter:
            small_to_delete, small_reasons = self._process_small_images(sorted_files, min_size)
            to_delete.update(small_to_delete)
            removal_reasons.update(small_reasons)
        
        # 2. é»‘ç™½å›¾è¿‡æ»¤
        if enable_grayscale_filter:
            gray_to_delete, gray_reasons = self._process_grayscale_images(sorted_files)
            # é¿å…é‡å¤æ·»åŠ 
            gray_to_delete = {img for img in gray_to_delete if img not in to_delete}
            to_delete.update(gray_to_delete)
            removal_reasons.update({k: v for k, v in gray_reasons.items() if k in gray_to_delete})
        
        # 3. é‡å¤å›¾ç‰‡è¿‡æ»¤
        if enable_duplicate_filter:
            # è·å–æœªè¢«å…¶ä»–è¿‡æ»¤å™¨åˆ é™¤çš„æ–‡ä»¶
            remaining_files = [f for f in sorted_files if f not in to_delete]
            if remaining_files:
                if duplicate_filter_mode == 'hash' and self.hash_file:
                    # ç›´æ¥ä½¿ç”¨å“ˆå¸Œæ–‡ä»¶è¿›è¡Œè¿‡æ»¤
                    hash_to_delete, hash_reasons = self._process_hash_images(remaining_files, ref_hamming_threshold)
                    to_delete.update(hash_to_delete)
                    removal_reasons.update(hash_reasons)
                else:
                    # ä½¿ç”¨ä¼ ç»Ÿçš„ç›¸ä¼¼å›¾ç‰‡ç»„è¿‡æ»¤
                    similar_groups = self._find_similar_images(remaining_files)
                    for group in similar_groups:
                        if len(group) <= 1:
                            continue
                            
                        if duplicate_filter_mode == 'watermark':
                            # æ°´å°è¿‡æ»¤æ¨¡å¼
                            watermark_to_delete, watermark_reasons = self._process_watermark_images(group, watermark_keywords)
                            to_delete.update(watermark_to_delete)
                            removal_reasons.update(watermark_reasons)
                        else:
                            # è´¨é‡è¿‡æ»¤æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
                            quality_to_delete, quality_reasons = self._process_quality_images(group)
                            to_delete.update(quality_to_delete)
                            removal_reasons.update(quality_reasons)
        
        # 4. çº¯æ–‡æœ¬å›¾ç‰‡è¿‡æ»¤
        if enable_text_filter:
            # è·å–æœªè¢«å…¶ä»–è¿‡æ»¤å™¨åˆ é™¤çš„æ–‡ä»¶
            remaining_files = [f for f in sorted_files if f not in to_delete]
            if remaining_files:
                text_to_delete, text_reasons = self._process_text_images(remaining_files, text_threshold)
                to_delete.update(text_to_delete)
                removal_reasons.update(text_reasons)
        
        return to_delete, removal_reasons

    def _find_similar_images(self, images: List[str]) -> List[List[str]]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„å›¾ç‰‡ç»„"""
        similar_groups = []
        processed = set()
        
        # å¹¶è¡Œè®¡ç®—æ‰€æœ‰å›¾ç‰‡çš„å“ˆå¸Œå€¼
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_img = {
                executor.submit(self._get_image_hash, img): img 
                for img in images
            }
            
            # æ”¶é›†å“ˆå¸Œå€¼
            hash_values = {}
            for future in as_completed(future_to_img):
                img = future_to_img[future]
                try:
                    hash_value = future.result()
                    if hash_value:
                        hash_values[img] = hash_value
                except Exception as e:
                    logger.error(f"è®¡ç®—å“ˆå¸Œå€¼å¤±è´¥ {img}: {e}")
        
        # ä½¿ç”¨åŠ é€Ÿå™¨è¿›è¡Œæ‰¹é‡æ¯”è¾ƒ
        target_hashes = list(hash_values.values())
        img_by_hash = {hash_val: img for img, hash_val in hash_values.items()}
        
        # æ‰¹é‡æŸ¥æ‰¾ç›¸ä¼¼å“ˆå¸Œ
        similar_results = HashAccelerator.batch_find_similar_hashes(
            target_hashes,
            target_hashes,
            img_by_hash,
            self.hamming_threshold
        )
        
        # å¤„ç†ç»“æœ,æ„å»ºç›¸ä¼¼å›¾ç‰‡ç»„
        for target_hash, similar_hashes in similar_results.items():
            if target_hash not in processed:
                current_group = [img_by_hash[target_hash]]
                processed.add(target_hash)
                
                for similar_hash, uri, distance in similar_hashes:
                    if similar_hash not in processed:
                        current_group.append(img_by_hash[similar_hash])
                        processed.add(similar_hash)
                        logger.info(f"æ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡: {os.path.basename(uri)} (è·ç¦»: {distance})")
                
                if len(current_group) > 1:
                    similar_groups.append(current_group)
                    logger.info(f"æ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡ç»„: {len(current_group)}å¼ ")
                
        return similar_groups

    def _compare_hashes(self, hash1: str, hash2: str, img2: str, threshold: int) -> bool:
        """æ¯”è¾ƒä¸¤ä¸ªå“ˆå¸Œå€¼"""
        try:
            if not hash2:
                return False
                
            # ä½¿ç”¨åŠ é€Ÿå™¨è®¡ç®—æ±‰æ˜è·ç¦»
            distances = HashAccelerator.calculate_hamming_distances(hash1, [hash2])
            if distances.size > 0 and distances[0] <= threshold:
                logger.info(f"æ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡: {os.path.basename(img2)} (è·ç¦»: {distances[0]})")
                return True
            return False
            
        except Exception as e:
            logger.error(f"æ¯”è¾ƒå“ˆå¸Œå€¼å¤±è´¥: {e}")
            return False

    def _get_image_hash(self, image_path: str) -> str:
        """è·å–å›¾ç‰‡å“ˆå¸Œå€¼ï¼Œä¼˜å…ˆä»ç¼“å­˜è¯»å–"""
        try:
            # å¢åŠ è·¯å¾„æœ‰æ•ˆæ€§æ£€æŸ¥
            if not image_path:
                logger.error("å›¾ç‰‡è·¯å¾„ä¸ºç©º")
                return None
                
            if not os.path.exists(image_path):
                logger.error(f"å›¾ç‰‡è·¯å¾„ä¸å­˜åœ¨: {image_path}")
                return None
            
            image_uri = PathURIGenerator.generate(image_path)
            if not image_uri:  # å¤„ç†ç”ŸæˆURIå¤±è´¥çš„æƒ…å†µ
                logger.error(f"ç”Ÿæˆå›¾ç‰‡URIå¤±è´¥: {image_path}")
                return None

            # å¢åŠ ç¼“å­˜é”®å­˜åœ¨æ€§æ£€æŸ¥
            if self.hash_cache:
                if image_uri in self.hash_cache:
                    hash_data = self.hash_cache[image_uri]
                    # å¤„ç†ä¸åŒçš„ç¼“å­˜æ•°æ®ç»“æ„
                    if isinstance(hash_data, dict):
                        hash_value = hash_data.get('hash')
                        if hash_value:
                            return hash_value
                    elif hash_data:  # å…¼å®¹æ—§ç‰ˆæœ¬å­—ç¬¦ä¸²æ ¼å¼
                        return str(hash_data)

            # ä½¿ç”¨ä¼˜åŒ–çš„æ–‡ä»¶è¯»å–
            img_data = self._read_file_optimized(image_path)
            if not img_data:
                logger.error(f"è¯»å–å›¾ç‰‡æ–‡ä»¶å¤±è´¥: {image_path}")
                return None

            # è®¡ç®—æ–°å“ˆå¸Œ
            hash_result = ImageHashCalculator.calculate_phash(BytesIO(img_data))
            if not hash_result:
                logger.error(f"è®¡ç®—å›¾ç‰‡å“ˆå¸Œå¤±è´¥: {image_path}")
                return None
                
            hash_value = hash_result.get('hash') if isinstance(hash_result, dict) else hash_result
            if not hash_value:
                logger.error(f"è·å–å“ˆå¸Œå€¼å¤±è´¥: {image_path}")
                return None

            # æ›´æ–°ç¼“å­˜
            self.hash_cache[image_uri] = {'hash': hash_value}
            return hash_value
        
        except Exception as e:
            logger.error(f"è·å–å›¾ç‰‡å“ˆå¸Œå¼‚å¸¸ {image_path}: {str(e)}")
            return None

    def _apply_watermark_filter(self, group: List[str], watermark_keywords: List[str] = None) -> List[Tuple[str, List[str]]]:
        """
        åº”ç”¨æ°´å°è¿‡æ»¤ï¼Œè¿”å›è¦åˆ é™¤çš„å›¾ç‰‡å’Œæ°´å°æ–‡å­—
        
        Args:
            group: ç›¸ä¼¼å›¾ç‰‡ç»„
            watermark_keywords: æ°´å°å…³é”®è¯åˆ—è¡¨ï¼ŒNoneæ—¶ä½¿ç”¨é»˜è®¤åˆ—è¡¨
        """
        to_delete = []
        watermark_results = {}
        
        # æ£€æµ‹æ¯å¼ å›¾ç‰‡çš„æ°´å°
        for img_path in group:
            has_watermark, texts = self.watermark_detector.detect_watermark(img_path, watermark_keywords)
            watermark_results[img_path] = (has_watermark, texts)
            if has_watermark:
                logger.info(f"å‘ç°æ°´å°: {os.path.basename(img_path)} -> {texts}")
            
        # æ‰¾å‡ºæ— æ°´å°çš„å›¾ç‰‡
        clean_images = [img for img, (has_mark, _) in watermark_results.items() if not has_mark]
        
        if clean_images:
            # å¦‚æœæœ‰æ— æ°´å°å›¾ç‰‡ï¼Œä¿ç•™å…¶ä¸­æœ€å¤§çš„ä¸€å¼ 
            keep_image = max(clean_images, key=lambda x: os.path.getsize(x))
            # åˆ é™¤å…¶ä»–æœ‰æ°´å°çš„å›¾ç‰‡
            for img in group:
                if img != keep_image and watermark_results[img][0]:
                    to_delete.append((img, watermark_results[img][1]))
                    
        return to_delete

    def _apply_quality_filter(self, group: List[str]) -> List[Tuple[str, str]]:
        """åº”ç”¨è´¨é‡è¿‡æ»¤ï¼ˆåŸºäºæ–‡ä»¶å¤§å°ï¼‰ï¼Œè¿”å›è¦åˆ é™¤çš„å›¾ç‰‡å’Œå¤§å°å·®å¼‚"""
        to_delete = []
        # è·å–æ–‡ä»¶å¤§å°
        file_sizes = {img: os.path.getsize(img) for img in group}
        # ä¿ç•™æœ€å¤§çš„æ–‡ä»¶
        keep_image = max(group, key=lambda x: file_sizes[x])
        
        # åˆ é™¤å…¶ä»–è¾ƒå°çš„æ–‡ä»¶
        for img in group:
            if img != keep_image:
                size_diff = f"{file_sizes[keep_image] - file_sizes[img]} bytes"
                to_delete.append((img, size_diff))
                
        return to_delete

    def detect_small_image(self, image_data, params):
        """ç‹¬ç«‹çš„å°å›¾æ£€æµ‹
        
        Args:
            image_data: PIL.Imageå¯¹è±¡æˆ–å›¾ç‰‡å­—èŠ‚æ•°æ®
            params: å‚æ•°å­—å…¸ï¼ŒåŒ…å«min_sizeç­‰é…ç½®
            
        Returns:
            Tuple[Union[bytes, None], Union[str, None]]: (å¤„ç†åçš„å›¾ç‰‡æ•°æ®, é”™è¯¯åŸå› )
        """
        try:
            # ç»Ÿä¸€è½¬æ¢ä¸ºPIL Imageå¯¹è±¡
            if isinstance(image_data, Image.Image):
                img = image_data
            else:
                img = Image.open(BytesIO(image_data))
                
            # è·å–å›¾ç‰‡å°ºå¯¸
            width, height = img.size
            min_size = params.get('min_size', 631)
            
            # æ£€æŸ¥å°ºå¯¸
            if width < min_size or height < min_size:
                logger.info(f"[#image_processing]ğŸ–¼ï¸ å›¾ç‰‡å°ºå¯¸: {width}x{height} å°äºæœ€å°å°ºå¯¸ {min_size}")
                return None, 'small_image'
                
            logger.info(f"[#image_processing]ğŸ–¼ï¸ å›¾ç‰‡å°ºå¯¸: {width}x{height} å¤§äºæœ€å°å°ºå¯¸ {min_size}")
            
            # å¦‚æœè¾“å…¥æ˜¯å­—èŠ‚æ•°æ®ï¼Œè¿”å›å­—èŠ‚æ•°æ®ï¼›å¦‚æœæ˜¯PIL Imageï¼Œè¿”å›åŸå¯¹è±¡
            if isinstance(image_data, Image.Image):
                return image_data, None
            else:
                img_byte_arr = BytesIO()
                img.save(img_byte_arr, format=img.format or 'PNG')
                return img_byte_arr.getvalue(), None
                
        except Exception as e:
            logger.error(f"æ£€æµ‹å›¾ç‰‡å°ºå¯¸æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None, 'size_detection_error'

    def detect_grayscale_image(self, image_data):
        """ç‹¬ç«‹çš„ç°åº¦å›¾å’Œçº¯è‰²å›¾æ£€æµ‹
        
        Args:
            image_data: PIL.Imageå¯¹è±¡æˆ–å›¾ç‰‡å­—èŠ‚æ•°æ®
            
        Returns:
            Tuple[Union[bytes, None], Union[str, None]]: (å¤„ç†åçš„å›¾ç‰‡æ•°æ®, é”™è¯¯åŸå› )
        """
        try:
            # ç»Ÿä¸€è½¬æ¢ä¸ºPIL Imageå¯¹è±¡
            if isinstance(image_data, Image.Image):
                img = image_data
            else:
                img = Image.open(BytesIO(image_data))
            
            # è½¬æ¢ä¸ºRGBæ¨¡å¼
            if img.mode not in ["RGB", "RGBA", "L"]:
                img = img.convert("RGB")
            
            # 1. æ£€æŸ¥æ˜¯å¦ä¸ºåŸå§‹ç°åº¦å›¾
            if img.mode == "L":
                logger.info("[#image_processing]ğŸ–¼ï¸ æ£€æµ‹åˆ°åŸå§‹ç°åº¦å›¾")
                return None, 'grayscale'
            
            # 2. è·å–å›¾ç‰‡çš„é‡‡æ ·ç‚¹è¿›è¡Œåˆ†æ
            width, height = img.size
            sample_points = [
                (x, y) 
                for x in range(0, width, max(1, width//10))
                for y in range(0, height, max(1, height//10))
            ][:100]  # æœ€å¤šå–100ä¸ªé‡‡æ ·ç‚¹
            
            # è·å–é‡‡æ ·ç‚¹çš„åƒç´ å€¼
            pixels = [img.getpixel(point) for point in sample_points]
            
            # 3. æ£€æŸ¥æ˜¯å¦ä¸ºçº¯ç™½å›¾
            if all(all(v > 240 for v in (pixel if isinstance(pixel, tuple) else (pixel,))) 
                   for pixel in pixels):
                logger.info("[#image_processing]ğŸ–¼ï¸ æ£€æµ‹åˆ°çº¯ç™½å›¾")
                return None, 'pure_white'
            
            # 4. æ£€æŸ¥æ˜¯å¦ä¸ºçº¯é»‘å›¾
            if all(all(v < 15 for v in (pixel if isinstance(pixel, tuple) else (pixel,))) 
                   for pixel in pixels):
                logger.info("[#image_processing]ğŸ–¼ï¸ æ£€æµ‹åˆ°çº¯é»‘å›¾")
                return None, 'pure_black'
            
            # 5. æ£€æŸ¥æ˜¯å¦ä¸ºç°åº¦å›¾
            if img.mode in ["RGB", "RGBA"]:
                is_grayscale = all(
                    abs(pixel[0] - pixel[1]) < 5 and 
                    abs(pixel[1] - pixel[2]) < 5 and
                    abs(pixel[0] - pixel[2]) < 5 
                    for pixel in pixels
                )
                if is_grayscale:
                    logger.info("[#image_processing]ğŸ–¼ï¸ æ£€æµ‹åˆ°ç°åº¦å›¾(RGBæ¥è¿‘)")
                    return None, 'grayscale'
            
            # è¿”å›åŸå§‹æ•°æ®
            if isinstance(image_data, Image.Image):
                return image_data, None
            else:
                img_byte_arr = BytesIO()
                img.save(img_byte_arr, format=img.format or 'PNG')
                return img_byte_arr.getvalue(), None
                
        except Exception as e:
            logger.error(f"æ£€æµ‹ç°åº¦å›¾æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None, 'grayscale_detection_error'