"""
å›¾ç‰‡å“ˆå¸Œè®¡ç®—æ¨¡å—
æä¾›å¤šç§å›¾ç‰‡å“ˆå¸Œè®¡ç®—æ–¹æ³•å’Œç›¸ä¼¼åº¦æ¯”è¾ƒåŠŸèƒ½
"""

from PIL import Image
import pillow_avif
import pillow_jxl
import cv2
import numpy as np
import logging
from io import BytesIO
from pathlib import Path
import imagehash
from itertools import combinations
from rich.markdown import Markdown
from rich.console import Console
from datetime import datetime

import orjson
import os
from urllib.parse import quote, unquote, urlparse
from dataclasses import dataclass
from typing import Dict, Tuple, Union, List, Optional
import re
from functools import lru_cache
import time
from nodes.pics.hash.path_uri import PathURIGenerator 
from nodes.pics.hash.image_clarity import ImageClarityEvaluator
from nodes.record.logger_config import setup_logger

# config = {
#     'script_name': 'calculate_hash_custom',
#     'console_enabled': False
# }
# logger, config_info = setup_logger(config)
logger = logging.getLogger(__name__)
# å¯¼å‡ºè¿™äº›ç±»ï¼Œä½¿å…¶ä¿æŒå‘åå…¼å®¹
__all__ = [
    'PathURIGenerator',
    'ImageClarityEvaluator',
    'ImageHashCalculator',
    'HashCache',
]
# å…¨å±€é…ç½®
GLOBAL_HASH_FILES = [
    os.path.expanduser(r"E:\1EHV\image_hashes_collection.json"),
    os.path.expanduser(r"E:\1EHV\image_hashes_global.json")
]
CACHE_TIMEOUT = 1800  # ç¼“å­˜è¶…æ—¶æ—¶é—´(ç§’)
HASH_FILES_LIST=os.path.expanduser(r"E:\1EHV\hash_files_list.txt")
# å“ˆå¸Œè®¡ç®—å‚æ•°
HASH_PARAMS = {
    'hash_size': 10,  # é»˜è®¤å“ˆå¸Œå¤§å°
    'hash_version': 1  # å“ˆå¸Œç‰ˆæœ¬å·ï¼Œç”¨äºåç»­å…¼å®¹æ€§å¤„ç†
}

class HashCache:
    """å“ˆå¸Œå€¼ç¼“å­˜ç®¡ç†ç±»ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    _instance = None
    _cache = {}
    _initialized = False
    _last_refresh = 0
    _last_save = 0  # æ–°å¢ï¼šè®°å½•ä¸Šæ¬¡ä¿å­˜æ—¶é—´
    _hash_counter = 0  # æ–°å¢ï¼šå“ˆå¸Œè®¡ç®—è®¡æ•°å™¨

    def __new__(cls):
        if not cls._instance:
            cls._instance = super().__new__(cls)
        return cls._instance

    @classmethod
    def get_cache(cls):
        """è·å–å†…å­˜ä¸­çš„ç¼“å­˜æ•°æ®"""
        current_time = time.time()
        
        # å¦‚æœæœªåˆå§‹åŒ–æˆ–è€…è·ç¦»ä¸Šæ¬¡åˆ·æ–°è¶…è¿‡è¶…æ—¶æ—¶é—´ï¼Œåˆ™åˆ·æ–°ç¼“å­˜
        if not cls._initialized or (current_time - cls._last_refresh > CACHE_TIMEOUT):
            cls.refresh_cache()
            
        return cls._cache
    
    @classmethod
    def refresh_cache(cls):
        """åˆ·æ–°ç¼“å­˜å¹¶ä¿æŒå†…å­˜é©»ç•™"""
        try:
            new_cache = {}
            loaded_files = []
            
            for hash_file in GLOBAL_HASH_FILES:
                try:
                    if not os.path.exists(hash_file):
                        logger.debug(f"å“ˆå¸Œæ–‡ä»¶ä¸å­˜åœ¨: {hash_file}")
                        continue
                        
                    with open(hash_file, 'rb') as f:
                        data = orjson.loads(f.read())
                        if not data:
                            logger.debug(f"å“ˆå¸Œæ–‡ä»¶ä¸ºç©º: {hash_file}")
                            continue
                            
                        # å¤„ç†æ–°æ ¼å¼ (image_hashes_collection.json)
                        if "hashes" in data:
                            hashes = data["hashes"]
                            if not hashes:
                                logger.debug(f"å“ˆå¸Œæ•°æ®ä¸ºç©º: {hash_file}")
                                continue
                                
                            for uri, hash_data in hashes.items():
                                if isinstance(hash_data, dict):
                                    if hash_str := hash_data.get('hash'):
                                        new_cache[uri] = hash_str
                                else:
                                    new_cache[uri] = str(hash_data)
                                    
                        # å¤„ç†æ—§æ ¼å¼ (image_hashes_global.json)
                        else:
                            # æ’é™¤ç‰¹æ®Šé”®
                            special_keys = {'_hash_params', 'dry_run', 'input_paths'}
                            for k, v in data.items():
                                if k not in special_keys:
                                    if isinstance(v, dict):
                                        if hash_str := v.get('hash'):
                                            new_cache[k] = hash_str
                                    else:
                                        new_cache[k] = str(v)
                                        
                        loaded_files.append(hash_file)
                        logger.debug(f"ä» {hash_file} åŠ è½½äº† {len(new_cache) - len(cls._cache)} ä¸ªæ–°å“ˆå¸Œå€¼")
                        
                except Exception as e:
                    logger.error(f"åŠ è½½å“ˆå¸Œæ–‡ä»¶å¤±è´¥ {hash_file}: {e}")
                    continue
                    
            if loaded_files:
                cls._cache = new_cache  # ç›´æ¥æ›¿æ¢å¼•ç”¨ä¿è¯åŸå­æ€§
                cls._initialized = True
                cls._last_refresh = time.time()
                # logger.info(f"å“ˆå¸Œç¼“å­˜å·²æ›´æ–°ï¼Œå…± {len(cls._cache)} ä¸ªæ¡ç›®")
            else:
                logger.warning("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å“ˆå¸Œæ–‡ä»¶")
                
        except Exception as e:
            logger.error(f"åˆ·æ–°å“ˆå¸Œç¼“å­˜å¤±è´¥: {e}")
            if not cls._initialized:
                cls._cache = {}  # å¦‚æœæ˜¯é¦–æ¬¡åˆå§‹åŒ–å¤±è´¥ï¼Œç¡®ä¿æœ‰ä¸€ä¸ªç©ºç¼“å­˜
            # ä¿æŒç°æœ‰ç¼“å­˜ä¸å˜

    @classmethod
    def sync_to_file(cls, force=False):
        """å°†å†…å­˜ç¼“å­˜åŒæ­¥åˆ°æ–‡ä»¶
        
        Args:
            force: æ˜¯å¦å¼ºåˆ¶åŒæ­¥ï¼Œå¿½ç•¥è®¡æ—¶å™¨å’Œè®¡æ•°å™¨
        
        Returns:
            bool: æ˜¯å¦æ‰§è¡Œäº†ä¿å­˜æ“ä½œ
        """
        current_time = time.time()
        should_save_by_time = (current_time - cls._last_save > 300)  # 5åˆ†é’Ÿä¿å­˜ä¸€æ¬¡
        should_save_by_count = (cls._hash_counter >= 10)  # ç´¯ç§¯10ä¸ªæ–°å“ˆå¸Œå€¼ä¿å­˜ä¸€æ¬¡
        
        if force or should_save_by_time or should_save_by_count:
            logger.info(f"åŒæ­¥å“ˆå¸Œç¼“å­˜åˆ°æ–‡ä»¶, å…±{len(cls._cache)}ä¸ªæ¡ç›® [è®¡æ•°:{cls._hash_counter}, é—´éš”:{int(current_time-cls._last_save)}ç§’]")
            ImageHashCalculator.save_global_hashes(cls._cache)
            cls._last_save = current_time
            cls._hash_counter = 0  # é‡ç½®è®¡æ•°å™¨
            return True
        
        return False

class ImgUtils:
    
    def get_img_files(directory):
        """è·å–ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶"""
        image_files = []
        image_extensions = ('.jpg', '.jpeg', '.png', '.webp', '.jxl', '.avif', '.bmp')
        
        for root, _, files in os.walk(directory):
            for file in files:
                if file.lower().endswith(image_extensions):
                    image_files.append(os.path.join(root, file))
                    
        return image_files
    
@dataclass
class ProcessResult:
    """å¤„ç†ç»“æœçš„æ•°æ®ç±»"""
    uri: str  # æ ‡å‡†åŒ–çš„URI
    hash_value: dict  # å›¾ç‰‡å“ˆå¸Œå€¼
    file_type: str  # æ–‡ä»¶ç±»å‹ï¼ˆ'image' æˆ– 'archive'ï¼‰
    original_path: str  # åŸå§‹æ–‡ä»¶è·¯å¾„

class ImageHashCalculator:
    """å›¾ç‰‡å“ˆå¸Œè®¡ç®—ç±»"""
    
    @staticmethod
    def normalize_path(path: str, internal_path: str = None) -> str:
        """æ ‡å‡†åŒ–è·¯å¾„ä¸ºURIæ ¼å¼
        
        Args:
            path: æ–‡ä»¶è·¯å¾„
            internal_path: å‹ç¼©åŒ…å†…éƒ¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: æ ‡å‡†åŒ–çš„URI
        """
        if internal_path:
            return PathURIGenerator.generate(f"{path}!{internal_path}")
        return PathURIGenerator.generate(path)

    @staticmethod
    def get_hash_from_url(url: str) -> Optional[str]:
        """
        æ ¹æ®URLæŸ¥è¯¢å…¨å±€å“ˆå¸Œå€¼
        Args:
            url: æ ‡å‡†åŒ–çš„URI
        Returns:
            str: å“ˆå¸Œå€¼å­—ç¬¦ä¸²ï¼Œæœªæ‰¾åˆ°è¿”å›None
        """
        try:
            if not url:
                logger.warning("URLä¸ºç©º")
                return None

            # æ ‡å‡†åŒ–URLæ ¼å¼
            normalized_url = PathURIGenerator.generate(url) if '://' not in url else url
            if not normalized_url:
                logger.warning(f"[#update_log]URLæ ‡å‡†åŒ–å¤±è´¥: {url}")
                return None
            
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            cached_hashes = HashCache.get_cache()
            if not cached_hashes:
                logger.debug("[#hash_calc]å“ˆå¸Œç¼“å­˜ä¸ºç©º")
                return None
                
            if hash_value := cached_hashes.get(normalized_url):
                logger.debug(f"[#hash_calc]ä»ç¼“å­˜æ‰¾åˆ°å“ˆå¸Œå€¼: {normalized_url}")
                return hash_value
                
            # æœªå‘½ä¸­ç¼“å­˜æ—¶ä¸»åŠ¨æ‰«æå…¨å±€æ–‡ä»¶
            for hash_file in GLOBAL_HASH_FILES:
                if not os.path.exists(hash_file):
                    logger.debug(f"[#hash_calc]å“ˆå¸Œæ–‡ä»¶ä¸å­˜åœ¨: {hash_file}")
                    continue
                    
                try:
                    with open(hash_file, 'rb') as f:
                        data = orjson.loads(f.read())
                        if not data:
                            logger.debug(f"[#hash_calc]å“ˆå¸Œæ–‡ä»¶ä¸ºç©º: {hash_file}")
                            continue
                            
                        # å¤„ç†æ–°æ—§æ ¼å¼
                        hashes = data.get('hashes', data) if 'hashes' in data else data
                        if not hashes:
                            logger.debug(f"[#hash_calc]å“ˆå¸Œæ•°æ®ä¸ºç©º: {hash_file}")
                            continue
                            
                        if hash_value := hashes.get(normalized_url):
                            if isinstance(hash_value, dict):
                                hash_str = hash_value.get('hash')
                                if hash_str:
                                    logger.debug(f"[#hash_calc]ä»å…¨å±€æ–‡ä»¶æ‰¾åˆ°å“ˆå¸Œå€¼: {normalized_url}")
                                    return hash_str
                            else:
                                logger.debug(f"[#hash_calc]ä»å…¨å±€æ–‡ä»¶æ‰¾åˆ°å“ˆå¸Œå€¼: {normalized_url}")
                                return str(hash_value)
                except Exception as e:
                    logger.warning(f"[#update_log]è¯»å–å“ˆå¸Œæ–‡ä»¶å¤±è´¥ {hash_file}: {e}")
                    continue
            
            logger.debug(f"[#hash_calc]æœªæ‰¾åˆ°å“ˆå¸Œå€¼: {normalized_url}")
            return None
            
        except Exception as e:
            logger.warning(f"[#update_log]æŸ¥è¯¢å“ˆå¸Œå¤±è´¥ {url}: {e}")
            return None

    @staticmethod
    def calculate_phash(image_path_or_data, hash_size=10, url=None, auto_save=True):
        """ä½¿ç”¨æ„ŸçŸ¥å“ˆå¸Œç®—æ³•è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼
        
        Args:
            image_path_or_data: å¯ä»¥æ˜¯å›¾ç‰‡è·¯å¾„(str/Path)ã€BytesIOå¯¹è±¡ã€byteså¯¹è±¡æˆ–PIL.Imageå¯¹è±¡
            hash_size: å“ˆå¸Œå¤§å°ï¼Œé»˜è®¤å€¼ä¸º10
            url: å›¾ç‰‡çš„URLï¼Œç”¨äºè®°å½•æ¥æºã€‚å¦‚æœä¸ºNoneä¸”image_path_or_dataæ˜¯è·¯å¾„ï¼Œåˆ™ä½¿ç”¨æ ‡å‡†åŒ–çš„URI
            auto_save: æ˜¯å¦è‡ªåŠ¨ä¿å­˜åˆ°å…¨å±€æ–‡ä»¶
            
        Returns:
            dict: åŒ…å«å“ˆå¸Œå€¼å’Œå…ƒæ•°æ®çš„å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›None
{
                    'hash': str,  # 16è¿›åˆ¶æ ¼å¼çš„æ„ŸçŸ¥å“ˆå¸Œå€¼
                    'size': int,  # å“ˆå¸Œå¤§å°
                    'url': str,   # æ ‡å‡†åŒ–çš„URI
                }
        """
        try:
            # ç”Ÿæˆæ ‡å‡†åŒ–çš„URI
            if url is None and isinstance(image_path_or_data, (str, Path)):
                path_str = str(image_path_or_data)
                url = PathURIGenerator.generate(path_str)
            
            # ä½¿ç”¨ç‹¬ç«‹å‡½æ•°æŸ¥è¯¢
            if cached_hash := ImageHashCalculator.get_hash_from_url(url):
                return {
                    'hash': cached_hash,
                    'size': HASH_PARAMS['hash_size'],
                    'url': url,
                    'from_cache': True
                }
            
            # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œåˆ™è®¡ç®—æ–°çš„å“ˆå¸Œå€¼
            # å¦‚æœæ²¡æœ‰æä¾›URLä¸”è¾“å…¥æ˜¯è·¯å¾„ï¼Œåˆ™ç”Ÿæˆæ ‡å‡†åŒ–çš„URI
            if url is None and isinstance(image_path_or_data, (str, Path)):
                path_str = str(image_path_or_data)
                url = PathURIGenerator.generate(path_str)  # ä½¿ç”¨æ–°ç±»ç”ŸæˆURI
                logger.debug(f"[#hash_calc]æ­£åœ¨è®¡ç®—URI: {url} çš„å“ˆå¸Œå€¼")
            
            # æ ¹æ®è¾“å…¥ç±»å‹é€‰æ‹©ä¸åŒçš„æ‰“å¼€æ–¹å¼
            if isinstance(image_path_or_data, (str, Path)):
                pil_img = Image.open(image_path_or_data)
            elif isinstance(image_path_or_data, BytesIO):
                pil_img = Image.open(image_path_or_data)
            elif isinstance(image_path_or_data, bytes):
                pil_img = Image.open(BytesIO(image_path_or_data))
            elif isinstance(image_path_or_data, Image.Image):
                pil_img = image_path_or_data
            elif hasattr(image_path_or_data, 'read') and hasattr(image_path_or_data, 'seek'):
                # æ”¯æŒmmapå’Œç±»æ–‡ä»¶å¯¹è±¡
                try:
                    # é¦–å…ˆå°è¯•ç›´æ¥è½¬æ¢ä¸ºBytesIOï¼ˆé€‚ç”¨äºmmapå¯¹è±¡ï¼‰
                    buffer = BytesIO(image_path_or_data)
                    pil_img = Image.open(buffer)
                except Exception as inner_e:
                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•è¯»å–å†…å®¹åå†è½¬æ¢
                    logger.debug(f"[#hash_calc]ç›´æ¥è½¬æ¢å¤±è´¥ï¼Œå°è¯•è¯»å–å†…å®¹: {inner_e}")
                    try:
                        position = image_path_or_data.tell()  # ä¿å­˜å½“å‰ä½ç½®
                        image_path_or_data.seek(0)  # å›åˆ°å¼€å¤´
                        content = image_path_or_data.read()  # è¯»å–å…¨éƒ¨å†…å®¹
                        image_path_or_data.seek(position)  # æ¢å¤ä½ç½®
                        pil_img = Image.open(BytesIO(content))
                    except Exception as e2:
                        raise ValueError(f"æ— æ³•ä»ç±»æ–‡ä»¶å¯¹è±¡è¯»å–å›¾ç‰‡æ•°æ®: {e2}")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {type(image_path_or_data)}")
            
            # ä½¿ç”¨imagehashåº“çš„phashå®ç°
            hash_obj = imagehash.phash(pil_img, hash_size=hash_size)
            
            # åªåœ¨æ‰“å¼€æ–°å›¾ç‰‡æ—¶å…³é—­
            if not isinstance(image_path_or_data, Image.Image):
                pil_img.close()
            
            # è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
            hash_str = str(hash_obj)
            
            if not hash_str:
                raise ValueError("ç”Ÿæˆçš„å“ˆå¸Œå€¼ä¸ºç©º")
                
            # å°†æ–°ç»“æœå­˜å…¥å†…å­˜ç¼“å­˜
            HashCache._cache[url] = hash_str
            
            # æ–°å¢ï¼šå¢åŠ å“ˆå¸Œè®¡æ•°å™¨å¹¶å°è¯•è‡ªåŠ¨ä¿å­˜
            if auto_save:
                HashCache._hash_counter += 1
                HashCache.sync_to_file()
                
            logger.debug(f"è®¡ç®—çš„å“ˆå¸Œå€¼: {hash_str}")
            return {
                'hash': hash_str,
                'size': hash_size,
                'url': url,
                'from_cache': False
            }
            
        except Exception as e:
            logger.warning(f"è®¡ç®—å¤±è´¥: {e}")
            return None

    @staticmethod
    def calculate_hamming_distance(hash1, hash2):
        """è®¡ç®—ä¸¤ä¸ªå“ˆå¸Œå€¼ä¹‹é—´çš„æ±‰æ˜è·ç¦»
        
        Args:
            hash1: ç¬¬ä¸€ä¸ªå“ˆå¸Œå€¼ï¼ˆå¯ä»¥æ˜¯å­—å…¸æ ¼å¼æˆ–å­—ç¬¦ä¸²æ ¼å¼ï¼‰
            hash2: ç¬¬äºŒä¸ªå“ˆå¸Œå€¼ï¼ˆå¯ä»¥æ˜¯å­—å…¸æ ¼å¼æˆ–å­—ç¬¦ä¸²æ ¼å¼ï¼‰
            
        Returns:
            int: æ±‰æ˜è·ç¦»ï¼Œå¦‚æœè®¡ç®—å¤±è´¥åˆ™è¿”å›float('inf')
        """
        try:
            # æ–°å¢ä»£ç ï¼šç»Ÿä¸€è½¬æ¢ä¸ºå°å†™
            hash1_str = hash1['hash'].lower() if isinstance(hash1, dict) else hash1.lower()
            hash2_str = hash2['hash'].lower() if isinstance(hash2, dict) else hash2.lower()
            
            # ç¡®ä¿ä¸¤ä¸ªå“ˆå¸Œå€¼é•¿åº¦ç›¸åŒ
            if len(hash1_str) != len(hash2_str):
                logger.info(f"å“ˆå¸Œé•¿åº¦ä¸ä¸€è‡´: {len(hash1_str)} vs {len(hash2_str)}")
                return float('inf')
            
            # å°†åå…­è¿›åˆ¶å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°
            hash1_int = int(hash1_str, 16)
            hash2_int = int(hash2_str, 16)
            
            # è®¡ç®—å¼‚æˆ–å€¼
            xor = hash1_int ^ hash2_int
            
            # ä½¿ç”¨Python 3.10+çš„bit_count()æ–¹æ³•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(int, 'bit_count'):
                distance = xor.bit_count()
            else:
                # ä¼˜åŒ–çš„åˆ†æ²»æ³•å®ç°
                x = xor
                x = (x & 0x5555555555555555) + ((x >> 1) & 0x5555555555555555)  # æ¯2ä½åˆ†ç»„
                x = (x & 0x3333333333333333) + ((x >> 2) & 0x3333333333333333)  # æ¯4ä½åˆ†ç»„
                x = (x & 0x0F0F0F0F0F0F0F0F) + ((x >> 4) & 0x0F0F0F0F0F0F0F0F)  # æ¯8ä½åˆ†ç»„
                # ç”±äºå“ˆå¸Œå€¼ä¸è¶…è¿‡64ä½ï¼Œå¯ä»¥ç›´æ¥ç´¯åŠ é«˜ä½
                x = (x + (x >> 8)) & 0x00FF00FF00FF00FF  # ç´¯åŠ æ¯ä¸ªå­—èŠ‚
                x = (x + (x >> 16)) & 0x0000FFFF0000FFFF  # ç´¯åŠ æ¯2ä¸ªå­—èŠ‚
                distance = (x + (x >> 32)) & 0x7F  # æœ€ç»ˆç»“æœä¸ä¼šè¶…è¿‡64
            
            logger.info(f"æ¯”è¾ƒå“ˆå¸Œå€¼: {hash1_str} vs {hash2_str}, æ±‰æ˜è·ç¦»: {distance}")
            return distance
            
        except Exception as e:
            logger.info(f"è®¡ç®—æ±‰æ˜è·ç¦»æ—¶å‡ºé”™: {e}")
            return float('inf')

    @staticmethod
    def match_existing_hashes(path: Path, existing_hashes: Dict[str, dict], is_global: bool = False) -> Dict[str, ProcessResult]:
        """åŒ¹é…è·¯å¾„ä¸ç°æœ‰å“ˆå¸Œå€¼"""
        results = {}
        # if 'å»å›¾' in path:
        #     return results
        if not existing_hashes:
            return results
            
        file_path = str(path).replace('\\', '/')
        
        # ç»Ÿä¸€ä½¿ç”¨åŒ…å«åŒ¹é…
        for uri, hash_value in existing_hashes.items():
            if file_path in uri:
                # å¦‚æœæ˜¯å…¨å±€å“ˆå¸Œï¼Œhash_valueæ˜¯å­—ç¬¦ä¸²ï¼›å¦‚æœæ˜¯æœ¬åœ°å“ˆå¸Œï¼Œhash_valueæ˜¯å­—å…¸
                if isinstance(hash_value, str):
                    hash_str = hash_value
                else:
                    hash_str = hash_value.get('hash', '')
                    
                file_type = 'archive' if '!' in uri else 'image'
                results[uri] = ProcessResult(
                    uri=uri,
                    hash_value={'hash': hash_str, 'size': HASH_PARAMS['hash_size'], 'url': uri},
                    file_type=file_type,
                    original_path=file_path
                )
                # æ ¹æ®æ¥æºæ˜¾ç¤ºä¸åŒçš„æ—¥å¿—
                log_prefix = "[ğŸŒå…¨å±€ç¼“å­˜]" if is_global else "[ğŸ“æœ¬åœ°ç¼“å­˜]"
                logger.info(f"[#hash_calc]{log_prefix} {file_type}: {file_path}  å“ˆå¸Œå€¼: {hash_str}")
        
        if results:
            logger.info(f"[#hash_calc]âœ… ä½¿ç”¨ç°æœ‰å“ˆå¸Œæ–‡ä»¶çš„ç»“æœï¼Œè·³è¿‡å¤„ç†")
            logger.info(f"[#current_progress]å¤„ç†è¿›åº¦: [å·²å®Œæˆ] ä½¿ç”¨ç°æœ‰å“ˆå¸Œ")
            
        return results



    @staticmethod
    def are_images_similar(hash1_str, hash2_str, threshold=2):
        """åˆ¤æ–­ä¸¤ä¸ªå›¾ç‰‡æ˜¯å¦ç›¸ä¼¼
        
        Args:
            hash1_str: ç¬¬ä¸€ä¸ªå›¾ç‰‡çš„å“ˆå¸Œå€¼
            hash2_str: ç¬¬äºŒä¸ªå›¾ç‰‡çš„å“ˆå¸Œå€¼
            threshold: æ±‰æ˜è·ç¦»é˜ˆå€¼ï¼Œå°äºç­‰äºæ­¤å€¼è®¤ä¸ºç›¸ä¼¼
            
        Returns:
            bool: æ˜¯å¦ç›¸ä¼¼
        """
        distance = ImageHashCalculator.calculate_hamming_distance(hash1_str, hash2_str)
        return distance <= threshold 

    @staticmethod
    def compare_folder_images(folder_path, hash_type='phash', threshold=2, output_html=None):
        """æ”¹è¿›ç‰ˆï¼šå¢åŠ å°ºå¯¸å’Œæ¸…æ™°åº¦å¯¹æ¯”"""
        console = Console()
        folder = Path(folder_path)
        image_exts = ('*.jpg', '*.jpeg', '*.png', '*.avif', '*.jxl', '*.webp', '*.JPG', '*.JPEG')
        image_files = [f for ext in image_exts for f in folder.glob(f'**/{ext}')]
        
        results = []
        # æ–°å¢ï¼šé¢„è®¡ç®—æ‰€æœ‰å›¾ç‰‡çš„å…ƒæ•°æ®
        meta_data = {}
        for img in image_files:
            width, height = ImageClarityEvaluator.get_image_size(img)
            meta_data[str(img)] = {
                'width': width,
                'height': height,
                'clarity': 0.0  # ç¨åå¡«å……
            }
        
        # æ‰¹é‡è®¡ç®—æ¸…æ™°åº¦
        clarity_scores = ImageClarityEvaluator.batch_evaluate(image_files)
        for path, score in clarity_scores.items():
            meta_data[path]['clarity'] = score
        
        for img1, img2 in combinations(image_files, 2):
            try:
                hash1 = getattr(ImageHashCalculator, f'calculate_{hash_type}')(img1)
                hash2 = getattr(ImageHashCalculator, f'calculate_{hash_type}')(img2)
                distance = ImageHashCalculator.calculate_hamming_distance(hash1, hash2)
                is_similar = distance <= threshold
                
                results.append({
                    'pair': (img1, img2),
                    'distance': distance,
                    'similar': is_similar
                })
            except Exception as e:
                logger.warning(f"å¯¹æ¯” {img1} å’Œ {img2} å¤±è´¥: {e}")
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_content = [
            '<!DOCTYPE html>',
            '<html><head>',
            '<meta charset="UTF-8">',
            '<title>å›¾ç‰‡ç›¸ä¼¼åº¦å¯¹æ¯”æŠ¥å‘Š</title>',
            '<style>',
            '  table {border-collapse: collapse; width: 100%; margin: 20px 0;}',
            '  th, td {border: 1px solid #ddd; padding: 12px; text-align: center;}',
            '  img {max-width: 200px; height: auto; transition: transform 0.3s;}',
            '  img:hover {transform: scale(1.5); cursor: zoom-in;}',
            '  .similar {color: #28a745;}',
            '  .different {color: #dc3545;}',
            '  body {font-family: Arial, sans-serif; margin: 30px;}',
            '</style></head><body>',
            '<h1>å›¾ç‰‡ç›¸ä¼¼åº¦å¯¹æ¯”æŠ¥å‘Š</h1>',
            f'<p><strong>å¯¹æ¯”æ—¶é—´</strong>ï¼š{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>',
            f'<p><strong>å“ˆå¸Œç®—æ³•</strong>ï¼š{hash_type.upper()}</p>',
            f'<p><strong>ç›¸ä¼¼é˜ˆå€¼</strong>ï¼š{threshold}</p>',
            '<table>',
            '  <tr><th>å›¾ç‰‡1</th><th>å›¾ç‰‡2</th><th>å°ºå¯¸</th><th>æ¸…æ™°åº¦</th><th>æ±‰æ˜è·ç¦»</th><th>ç›¸ä¼¼åˆ¤å®š</th></tr>'
        ]

        for res in results:
            status_class = 'similar' if res['similar'] else 'different'
            status_icon = 'âœ…' if res['similar'] else 'âŒ'
            img1_path = str(res['pair'][0].resolve()).replace('\\', '/')
            img2_path = str(res['pair'][1].resolve()).replace('\\', '/')
            img1_meta = meta_data[str(res['pair'][0])]
            img2_meta = meta_data[str(res['pair'][1])]
            
            html_content.append(
                f'<tr>'
                f'<td><img src="file:///{img1_path}" alt="{img1_path}"><br>{img1_meta["width"]}x{img1_meta["height"]}</td>'
                f'<td><img src="file:///{img2_path}" alt="{img2_path}"><br>{img2_meta["width"]}x{img2_meta["height"]}</td>'
                f'<td>{img1_meta["width"]}x{img1_meta["height"]} vs<br>{img2_meta["width"]}x{img2_meta["height"]}</td>'
                f'<td>{img1_meta["clarity"]:.1f} vs {img2_meta["clarity"]:.1f}</td>'
                f'<td>{res["distance"]}</td>'
                f'<td class="{status_class}">{status_icon} {"ç›¸ä¼¼" if res["similar"] else "ä¸ç›¸ä¼¼"}</td>'
                f'</tr>'
            )
            
        html_content.extend(['</table></body></html>'])
        
        # æ§åˆ¶å°ç®€åŒ–è¾“å‡º
        console.print(f"å®Œæˆå¯¹æ¯”ï¼Œå…±å¤„ç† {len(results)} ç»„å›¾ç‰‡å¯¹")
        
        if output_html:
            output_path = Path(output_html)
            output_path.write_text('\n'.join(html_content), encoding='utf-8')
            console.print(f"HTMLæŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š[bold green]{output_path.resolve()}[/]")
            console.print("æç¤ºï¼šåœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ–‡ä»¶å¯æŸ¥çœ‹äº¤äº’å¼å›¾ç‰‡ç¼©æ”¾æ•ˆæœ")

    @staticmethod
    def save_global_hashes(hash_dict: Dict[str, str]) -> None:
        """ä¿å­˜å“ˆå¸Œå€¼åˆ°å…¨å±€ç¼“å­˜æ–‡ä»¶ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            output_dict = {
                "_hash_params": f"hash_size={HASH_PARAMS['hash_size']};hash_version={HASH_PARAMS['hash_version']}",
                "hashes": hash_dict  # ç›´æ¥å­˜å‚¨å­—ç¬¦ä¸²å­—å…¸ï¼Œè·³è¿‡ä¸­é—´è½¬æ¢
            }
            
            os.makedirs(os.path.dirname(GLOBAL_HASH_FILES[-1]), exist_ok=True)
            with open(GLOBAL_HASH_FILES[-1], 'wb') as f:
                # ä½¿ç”¨orjsonçš„OPT_SERIALIZE_NUMPYé€‰é¡¹æå‡æ•°å€¼å¤„ç†æ€§èƒ½
                f.write(orjson.dumps(output_dict, 
                    option=orjson.OPT_INDENT_2 | 
                    orjson.OPT_SERIALIZE_NUMPY |
                    orjson.OPT_APPEND_NEWLINE))
            logger.debug(f"å·²ä¿å­˜å“ˆå¸Œç¼“å­˜åˆ°: {GLOBAL_HASH_FILES[-1]}")  # æ”¹ä¸ºdebugçº§åˆ«å‡å°‘æ—¥å¿—é‡
        except Exception as e:
            logger.warning(f"ä¿å­˜å…¨å±€å“ˆå¸Œç¼“å­˜å¤±è´¥: {e}", exc_info=True)

    @staticmethod
    def load_global_hashes() -> Dict[str, str]:
        """ä»å…¨å±€ç¼“å­˜æ–‡ä»¶åŠ è½½æ‰€æœ‰å“ˆå¸Œå€¼ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            if os.path.exists(GLOBAL_HASH_FILES[-1]):
                with open(GLOBAL_HASH_FILES[-1], 'rb') as f:
                    data = orjson.loads(f.read())
                    return {
                        uri: entry["hash"] if isinstance(entry, dict) else entry
                        for uri, entry in data.get("hashes", {}).items()
                    }
            return {}
        except Exception as e:
            logger.warning(f"åŠ è½½å…¨å±€å“ˆå¸Œç¼“å­˜å¤±è´¥: {e}", exc_info=True)
            return {}

    @staticmethod
    def save_hash_file_path(file_path: str) -> None:
        """å°†å“ˆå¸Œæ–‡ä»¶è·¯å¾„ä¿å­˜åˆ°è·¯å¾„é›†åˆæ–‡ä»¶ä¸­
        
        Args:
            file_path: è¦ä¿å­˜çš„å“ˆå¸Œæ–‡ä»¶è·¯å¾„
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(HASH_FILES_LIST), exist_ok=True)
            # è¿½åŠ æ¨¡å¼å†™å…¥è·¯å¾„
            with open(HASH_FILES_LIST, 'a', encoding='utf-8') as f:
                f.write(f"{file_path}\n")
            logger.info(f"å·²å°†å“ˆå¸Œæ–‡ä»¶è·¯å¾„ä¿å­˜åˆ°é›†åˆæ–‡ä»¶: {HASH_FILES_LIST}")
        except Exception as e:
            logger.error(f"ä¿å­˜å“ˆå¸Œæ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")

    @staticmethod
    def get_latest_hash_file_path() -> Optional[str]:
        """è·å–æœ€æ–°çš„å“ˆå¸Œæ–‡ä»¶è·¯å¾„
        
        Returns:
            Optional[str]: æœ€æ–°çš„å“ˆå¸Œæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
        """
        try:
            if not os.path.exists(HASH_FILES_LIST):
                return None
                
            with open(HASH_FILES_LIST, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            if not lines:
                return None
                
            # è·å–æœ€åä¸€è¡Œå¹¶å»é™¤ç©ºç™½å­—ç¬¦
            latest_path = lines[-1].strip()
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(latest_path):
                return latest_path
            else:
                logger.error(f"æœ€æ–°çš„å“ˆå¸Œæ–‡ä»¶ä¸å­˜åœ¨: {latest_path}")
                return None
                
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°å“ˆå¸Œæ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")
            return None

    @staticmethod
    def load_existing_hashes(directory: Path) -> Dict[str, str]:
        """æœ€ç»ˆä¿®å¤ç‰ˆå“ˆå¸ŒåŠ è½½"""
        existing_hashes = {}
        try:
            hash_file = directory / 'image_hashes.json'
            if not hash_file.exists():
                return existing_hashes
            
            with open(hash_file, 'rb') as f:
                data = orjson.loads(f.read())
                
                if 'results' in data:
                    results = data['results']
                    for uri, result in results.items():
                        # ä¿®å¤å­—æ®µæ˜ å°„é—®é¢˜
                        if isinstance(result, dict):
                            # ç»Ÿä¸€ä½¿ç”¨hashå­—æ®µ
                            hash_str = str(result.get('hash', ''))
                            # æ·»åŠ ç±»å‹éªŒè¯
                            if len(hash_str) >= 8:  # è°ƒæ•´ä¸ºæ›´å®½æ¾çš„é•¿åº¦éªŒè¯
                                existing_hashes[uri] = {
                                    'hash': hash_str.lower(),
                                    'size': HASH_PARAMS['hash_size'],
                                    'url': uri
                                }
                                continue
                        logger.warning(f"æ— æ•ˆçš„å“ˆå¸Œæ¡ç›®: {uri} - {result}")
                
                logger.info(f"ä» {hash_file} åŠ è½½åˆ°æœ‰æ•ˆæ¡ç›®: {len(existing_hashes)}")
                return existing_hashes
            
        except Exception as e:
            logger.error(f"åŠ è½½å“ˆå¸Œæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
            return {}

    @staticmethod
    def save_hash_results(results: Dict[str, ProcessResult], output_path: Path, dry_run: bool = False) -> None:
        """ä¿å­˜å“ˆå¸Œç»“æœåˆ°æ–‡ä»¶"""
        try:
            output = {
                "_hash_params": f"hash_size={HASH_PARAMS['hash_size']};hash_version={HASH_PARAMS['hash_version']}",
                "dry_run": dry_run,
                "hashes": {uri: {"hash": result.hash_value['hash']} for uri, result in results.items()}  # ä¸å…¨å±€ç»“æ„ä¸€è‡´
            }
            
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'wb') as f:
                f.write(orjson.dumps(output, option=orjson.OPT_INDENT_2))
            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path} (å…± {len(output['hashes'])} ä¸ªå“ˆå¸Œå€¼)")
            
            ImageHashCalculator.save_hash_file_path(str(output_path))
            
        except Exception as e:
            logger.error(f"ä¿å­˜å“ˆå¸Œç»“æœå¤±è´¥: {e}") 


    @staticmethod
    def load_hashes(file_path: Path) -> Tuple[Dict[str, str], dict]:
        """åŠ è½½å“ˆå¸Œæ–‡ä»¶ï¼ˆä»…å¤„ç†æ–°ç»“æ„ï¼‰"""
        try:
            with open(file_path, 'rb') as f:
                data = orjson.loads(f.read())
                hash_params = ImageHashCalculator.parse_hash_params(data.get('_hash_params', ''))
                return {
                    k: v['hash']  # æ–°ç»“æ„å¼ºåˆ¶è¦æ±‚hashå­—æ®µ
                    for k, v in data.get('hashes', {}).items()
                }, hash_params
        except Exception as e:
            logger.debug(f"å°è¯•æ–°ç»“æ„åŠ è½½å¤±è´¥ï¼Œå›é€€æ—§ç»“æ„: {e}")
            return LegacyHashLoader.load(file_path)  # åˆ†ç¦»çš„æ—§ç»“æ„åŠ è½½

    @staticmethod
    def migrate_hashes(file_path: Path) -> None:
        """è¿ç§»æ—§å“ˆå¸Œæ–‡ä»¶åˆ°æ–°æ ¼å¼"""
        hashes, params = ImageHashCalculator.load_hashes(file_path)
        if hashes:
            ImageHashCalculator.save_hash_results(
                results={uri: ProcessResult(h, None, None) for uri, h in hashes.items()},
                output_path=file_path,
                dry_run=False
            )
            logger.info(f"å·²è¿ç§»å“ˆå¸Œæ–‡ä»¶æ ¼å¼: {file_path}")

    @staticmethod
    def test_hash_cache():
        """ç¼“å­˜åŠŸèƒ½æµ‹è¯•demo"""
        console = Console()
        test_file = r"D:\1VSCODE\1ehv\pics\test\0.jpg"  # æ›¿æ¢ä¸ºå®é™…æµ‹è¯•æ–‡ä»¶è·¯å¾„
        url=ImageHashCalculator.normalize_path(test_file)
        # ç¬¬ä¸€æ¬¡è®¡ç®—ï¼ˆåº”åŠ è½½ç¼“å­˜ï¼‰
        console.print("\n[bold cyan]=== ç¬¬ä¸€æ¬¡è®¡ç®—ï¼ˆåŠ è½½ç¼“å­˜ï¼‰===[/]")
        start_time = time.time()
        hash1 = ImageHashCalculator.calculate_phash(test_file)
        load_hashes=ImageHashCalculator.load_hashes(test_file)
        console.print(f"è€—æ—¶: {time.time()-start_time:.2f}s")
        
        # ç¬¬äºŒæ¬¡è®¡ç®—ï¼ˆåº”ä½¿ç”¨ç¼“å­˜ï¼‰
        console.print("\n[bold cyan]=== ç¬¬äºŒæ¬¡è®¡ç®—ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰===[/]")
        start_time = time.time()
        hash2 = ImageHashCalculator.calculate_phash(test_file)
        console.print(f"è€—æ—¶: {time.time()-start_time:.2f}s")
        
        # éªŒè¯ç»“æœ
        console.print("\n[bold]æµ‹è¯•ç»“æœ:[/]")
        console.print(f"å“ˆå¸Œå€¼æ˜¯å¦ä¸€è‡´: {hash1['hash'] == hash2['hash']}")
        console.print(f"æ˜¯å¦æ¥è‡ªç¼“å­˜: {hash1.get('from_cache', False)} | {hash2.get('from_cache', False)}")

class LegacyHashLoader:
    """æ—§ç»“æ„å“ˆå¸Œæ–‡ä»¶åŠ è½½å™¨ï¼ˆåæœŸå¯æ•´ä½“ç§»é™¤ï¼‰"""
    
    @staticmethod
    def load(file_path: Path) -> Tuple[Dict[str, str], dict]:
        """åŠ è½½æ—§ç‰ˆå“ˆå¸Œæ–‡ä»¶ç»“æ„"""
        try:
            with open(file_path, 'rb') as f:
                data = orjson.loads(f.read())
                return LegacyHashLoader._parse_old_structure(data)
        except:
            return {}, {}
    @staticmethod
    def parse_hash_params(param_str: str) -> dict:
        """è§£æå“ˆå¸Œå‚æ•°å­—ç¬¦ä¸²"""
        params = {
            'hash_size': HASH_PARAMS['hash_size'],
            'hash_version': HASH_PARAMS['hash_version']
        }
        for pair in param_str.split(';'):
            if '=' in pair:
                key, val = pair.split('=', 1)
                if key in params:
                    params[key] = int(val)
        return params
    @staticmethod
    def _parse_old_structure(data: dict) -> Tuple[Dict[str, str], dict]:
        """è§£æä¸åŒæ—§ç‰ˆç»“æ„"""
        hash_params = ImageHashCalculator.parse_hash_params(data.get('_hash_params', ''))
        
        # ç‰ˆæœ¬1: åŒ…å«resultsçš„ç»“æ„
        if 'results' in data:
            return {
                uri: item.get('hash') or uri.split('[hash-')[1].split(']')[0]
                for uri, item in data['results'].items()
            }, hash_params
            
        # ç‰ˆæœ¬2: åŒ…å«filesçš„ç»“æ„
        if 'files' in data:
            return {
                k: v if isinstance(v, str) else v.get('hash', '')
                for k, v in data['files'].items()
            }, hash_params
            
        # ç‰ˆæœ¬3: æœ€æ—§å…¨å±€æ–‡ä»¶ç»“æ„
        return {
            k: v['hash'] if isinstance(v, dict) else v
            for k, v in data.items()
            if k not in ['_hash_params', 'dry_run', 'input_paths']
        }, hash_params 
        

if __name__ == "__main__":
    # æ‰§è¡Œç¼“å­˜æµ‹è¯•
    ImageHashCalculator.test_hash_cache()
    # åŸæœ‰æ¸…æ™°åº¦æµ‹è¯•ä¿æŒä¸å˜
    def test_image_clarity():
        """æ¸…æ™°åº¦è¯„ä¼°æµ‹è¯•demo"""
        test_dir = Path(r"D:\1VSCODE\1ehv\pics\test")
        console = Console()
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = list(test_dir.glob("*.jpg")) + list(test_dir.glob("*.png"))
        console.print(f"æ‰¾åˆ° {len(image_files)} å¼ æµ‹è¯•å›¾ç‰‡")
        
        # è®¡ç®—æ¸…æ™°åº¦å¹¶æ’åº
        results = []
        for img_path in image_files[:1300]:  # é™åˆ¶å‰1300å¼ 
            score = ImageClarityEvaluator.calculate_definition(img_path)
            results.append((img_path.name, score))
        
        # æŒ‰æ¸…æ™°åº¦é™åºæ’åº
        sorted_results = sorted(results, key=lambda x: x[1], reverse=True)
        
        # è¾“å‡ºç»“æœ
        console.print(Markdown("## å›¾åƒæ¸…æ™°åº¦æ’å"))
        console.print("| æ’å | æ–‡ä»¶å | æ¸…æ™°åº¦å¾—åˆ† |")
        console.print("|------|--------|------------|")
        for idx, (name, score) in enumerate(sorted_results[:20], 1):
            console.print(f"| {idx:2d} | {name} | {score:.2f} |")
            
    # æ‰§è¡Œæµ‹è¯•
    # test_image_clarity()

