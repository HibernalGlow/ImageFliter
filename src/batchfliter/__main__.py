import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, Any, List, Set, Tuple, Optional
from common.archive import ArchiveHandler,SUPPORTED_ARCHIVE_FORMATS
from common.input import InputHandler
from textual_preset import create_config_app
from .utils.merge import ArchiveMerger
from textual_logger import TextualLoggerManager
import shutil
import time
import logging
import subprocess
from pathlib import Path
from typing import List, Tuple, Optional
from send2trash import send2trash
# é…ç½®æ—¥å¿—
from loguru import logger
import os
import sys
from pathlib import Path
from datetime import datetime

def setup_logger(app_name="app", project_root=None, console_output=True):
    """é…ç½® Loguru æ—¥å¿—ç³»ç»Ÿ
    
    Args:
        app_name: åº”ç”¨åç§°ï¼Œç”¨äºæ—¥å¿—ç›®å½•
        project_root: é¡¹ç›®æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
        console_output: æ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œé»˜è®¤ä¸ºTrue
        
    Returns:
        tuple: (logger, config_info)
            - logger: é…ç½®å¥½çš„ logger å®ä¾‹
            - config_info: åŒ…å«æ—¥å¿—é…ç½®ä¿¡æ¯çš„å­—å…¸
    """
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    if project_root is None:
        project_root = Path(__file__).parent.resolve()
    
    # æ¸…é™¤é»˜è®¤å¤„ç†å™¨
    logger.remove()
    
    # æœ‰æ¡ä»¶åœ°æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ï¼ˆç®€æ´ç‰ˆæ ¼å¼ï¼‰
    if console_output:
        logger.add(
            sys.stdout,
            level="INFO",
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <blue>{elapsed}</blue> | <level>{level.icon} {level: <8}</level> | <cyan>{name}:{function}:{line}</cyan> - <level>{message}</level>"
        )
    
    # ä½¿ç”¨ datetime æ„å»ºæ—¥å¿—è·¯å¾„
    current_time = datetime.now()
    date_str = current_time.strftime("%Y-%m-%d")
    hour_str = current_time.strftime("%H")
    minute_str = current_time.strftime("%M%S")
    
    # æ„å»ºæ—¥å¿—ç›®å½•å’Œæ–‡ä»¶è·¯å¾„
    log_dir = os.path.join(project_root, "logs", app_name, date_str, hour_str)
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"{minute_str}.log")
    
    # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    logger.add(
        log_file,
        level="DEBUG",
        rotation="10 MB",
        retention="30 days",
        compression="zip",
        encoding="utf-8",
        format="{time:YYYY-MM-DD HH:mm:ss} | {elapsed} | {level.icon} {level: <8} | {name}:{function}:{line} - {message}",
    )
    
    # åˆ›å»ºé…ç½®ä¿¡æ¯å­—å…¸
    config_info = {
        'log_file': log_file,
    }
    
    logger.info(f"æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œåº”ç”¨åç§°: {app_name}")
    return logger, config_info

logger, config_info = setup_logger(app_name="batch_img_filter", console_output=False)


# TextualLoggerå¸ƒå±€é…ç½®
TEXTUAL_LAYOUT = {
    "cur_stats": {
        "ratio": 1,
        "title": "ğŸ“Š æ€»ä½“è¿›åº¦",
        "style": "lightyellow"
    },
    "cur_progress": {
        "ratio": 1,
        "title": "ğŸ”„ å½“å‰è¿›åº¦",
        "style": "lightcyan"
    },
    "file_ops": {
        "ratio": 2,
        "title": "ğŸ“‚ æ–‡ä»¶æ“ä½œ",
        "style": "lightpink"
    },
    "hash_calc": {
        "ratio": 2,
        "title": "ğŸ”¢ å“ˆå¸Œè®¡ç®—",
        "style": "lightblue"
    },
    "update_log": {
        "ratio": 1,
        "title": "ğŸ”§ ç³»ç»Ÿæ¶ˆæ¯",
        "style": "lightwhite"
    }
}

# åˆå§‹åŒ–TextualLogger
HAS_TUI = True

# å¸¸é‡å®šä¹‰
DEFAULT_MIN_SIZE = 631
DEFAULT_HAMMING_DISTANCE = 12

def initialize_textual_logger():
    """åˆå§‹åŒ–æ—¥å¿—å¸ƒå±€ï¼Œç¡®ä¿åœ¨æ‰€æœ‰æ¨¡å¼ä¸‹éƒ½èƒ½æ­£ç¡®åˆå§‹åŒ–"""
    try:
        TextualLoggerManager.set_layout(TEXTUAL_LAYOUT, config_info['log_file'])
        logger.info("[#update_log]âœ… æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
class ArchiveMerger:
    # é»‘åå•å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤ä¸éœ€è¦å¤„ç†çš„æ–‡ä»¶
    BLACKLIST_KEYWORDS = ['merged_', 'temp_', 'backup_', '.new', '.trash']
    
    @staticmethod
    def merge_archives(paths: List[str]) -> Tuple[Optional[str], Optional[str], List[str]]:
        """
        å°†å¤šä¸ªå‹ç¼©åŒ…åˆå¹¶ä¸ºä¸€ä¸ªä¸´æ—¶å‹ç¼©åŒ…
        
        Args:
            paths: å‹ç¼©åŒ…è·¯å¾„åˆ—è¡¨
            
        Returns:
            Tuple[str, str, List[str]]: (ä¸´æ—¶ç›®å½•è·¯å¾„, åˆå¹¶åçš„å‹ç¼©åŒ…è·¯å¾„, åŸå§‹å‹ç¼©åŒ…è·¯å¾„åˆ—è¡¨)
            å¦‚æœå¤±è´¥åˆ™è¿”å› (None, None, [])
            å¦‚æœåªæœ‰ä¸€ä¸ªå‹ç¼©åŒ…ï¼Œåˆ™è¿”å› (None, åŸå§‹å‹ç¼©åŒ…è·¯å¾„, [åŸå§‹å‹ç¼©åŒ…è·¯å¾„])
        """
        temp_dir = None
        try:
            # æ”¶é›†æ‰€æœ‰ZIPæ–‡ä»¶è·¯å¾„ï¼ŒåŒæ—¶æ’é™¤é»‘åå•ä¸­çš„å…³é”®è¯
            archive_paths = []
            for path in paths:
                # æ£€æŸ¥è·¯å¾„æ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯
                if any(keyword in path for keyword in ArchiveMerger.BLACKLIST_KEYWORDS):
                    logger.info(f"[#file_ops]è·³è¿‡é»‘åå•æ–‡ä»¶: {path}")
                    continue
                    
                if os.path.isdir(path):
                    for root, _, files in os.walk(path):
                        for f in files:
                            file_path = os.path.join(root, f)
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ˜¯zipå¹¶ä¸”ä¸åœ¨é»‘åå•ä¸­
                            if f.lower().endswith('.zip') and not any(keyword in f for keyword in ArchiveMerger.BLACKLIST_KEYWORDS):
                                archive_paths.append(file_path)
                            elif f.lower().endswith('.zip'):
                                logger.info(f"[#file_ops]è·³è¿‡é»‘åå•å‹ç¼©åŒ…: {f}")
                elif path.lower().endswith('.zip'):
                    archive_paths.append(path)
            
            if not archive_paths:
                logger.info("[#file_ops]æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„å‹ç¼©åŒ…")
                return (None, None, [])
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªå‹ç¼©åŒ…ï¼Œç›´æ¥è¿”å›å®ƒ
            if len(archive_paths) == 1:
                logger.info(f"[#file_ops]åªæœ‰ä¸€ä¸ªå‹ç¼©åŒ…ï¼Œæ— éœ€åˆå¹¶: {archive_paths[0]}")
                return (None, archive_paths[0], archive_paths)
            
            # ç¡®ä¿æ‰€æœ‰å‹ç¼©åŒ…åœ¨åŒä¸€ç›®å½•
            directories = {os.path.dirname(path) for path in archive_paths}
            if len(directories) > 1:
                logger.info("[#file_ops]æ‰€é€‰å‹ç¼©åŒ…ä¸åœ¨åŒä¸€ç›®å½•")
                return (None, None, [])
                
            base_dir = list(directories)[0]
            timestamp = int(time.time() * 1000)
            temp_dir = os.path.join(base_dir, f'temp_merge_{timestamp}')
            os.makedirs(temp_dir, exist_ok=True)
            
            # è§£å‹æ‰€æœ‰å‹ç¼©åŒ…
            for zip_path in archive_paths:
                logger.info(f'[#file_ops]è§£å‹: {zip_path}')
                archive_name = os.path.splitext(os.path.basename(zip_path))[0]
                archive_temp_dir = os.path.join(temp_dir, archive_name)
                os.makedirs(archive_temp_dir, exist_ok=True)
                
                cmd = ['7z', 'x', zip_path, f'-o{archive_temp_dir}', '-y']
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.returncode != 0:
                    logger.info(f"[#file_ops]è§£å‹å¤±è´¥: {zip_path}\né”™è¯¯: {result.stderr}")
                    return (None, None, [])
            
            # åˆ›å»ºåˆå¹¶åçš„å‹ç¼©åŒ…
            merged_zip_path = os.path.join(base_dir, f'merged_{timestamp}.zip')
            logger.info('[#file_ops]åˆ›å»ºåˆå¹¶å‹ç¼©åŒ…')
            
            cmd = ['7z', 'a', '-tzip', merged_zip_path, os.path.join(temp_dir, '*')]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                logger.info(f"[#file_ops]åˆ›å»ºåˆå¹¶å‹ç¼©åŒ…å¤±è´¥: {result.stderr}")
                return (None, None, [])
                
            return (temp_dir, merged_zip_path, archive_paths)
            
        except Exception as e:
            logger.info(f"[#file_ops]åˆå¹¶å‹ç¼©åŒ…æ—¶å‡ºé”™: {e}")
            if temp_dir and os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
            return (None, None, [])
    @staticmethod
    def split_merged_archive(processed_zip, original_archives, temp_dir, params):
        """
        å°†å¤„ç†åçš„åˆå¹¶å‹ç¼©åŒ…æ‹†åˆ†å›åŸå§‹å‹ç¼©åŒ…
        
        Args:
            processed_zip: å¤„ç†åçš„åˆå¹¶å‹ç¼©åŒ…è·¯å¾„
            original_archives: åŸå§‹å‹ç¼©åŒ…è·¯å¾„åˆ—è¡¨
            temp_dir: ä¸´æ—¶ç›®å½•è·¯å¾„
            params: å‚æ•°å­—å…¸
        """
        try:
            logger.info('å¼€å§‹æ‹†åˆ†å¤„ç†åçš„å‹ç¼©åŒ…')
            extract_dir = os.path.join(temp_dir, 'processed')
            os.makedirs(extract_dir, exist_ok=True)
            
            # è§£å‹å¤„ç†åçš„å‹ç¼©åŒ…
            cmd = ['7z', 'x', processed_zip, f'-o{extract_dir}', '-y']
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                logger.info(f"âŒ è§£å‹å¤„ç†åçš„å‹ç¼©åŒ…å¤±è´¥: {result.stderr}")
                return False
                
            for original_zip in original_archives:
                archive_name = os.path.splitext(os.path.basename(original_zip))[0]
                source_dir = os.path.join(extract_dir, archive_name)
                
                if not os.path.exists(source_dir):
                    logger.info(f"âš ï¸ æ‰¾ä¸åˆ°å¯¹åº”çš„ç›®å½•: {source_dir}")
                    continue
                    
                new_zip = original_zip + '.new'
                
                # åˆ›å»ºæ–°å‹ç¼©åŒ…
                cmd = ['7z', 'a', '-tzip', new_zip, os.path.join(source_dir, '*')]
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.returncode == 0:
                    try:
                        # é»˜è®¤ä½¿ç”¨å›æ”¶ç«™åˆ é™¤
                        send2trash(original_zip)
                        os.rename(new_zip, original_zip)
                        logger.info(f'æˆåŠŸæ›´æ–°å‹ç¼©åŒ…: {original_zip}')
                    except Exception as e:
                        logger.info(f"âŒ æ›¿æ¢å‹ç¼©åŒ…å¤±è´¥ {original_zip}: {e}")
                else:
                    logger.info(f"âŒ åˆ›å»ºæ–°å‹ç¼©åŒ…å¤±è´¥ {new_zip}: {result.stderr}")
            
            return True
        except Exception as e:
            logger.info(f"âŒ æ‹†åˆ†å‹ç¼©åŒ…æ—¶å‡ºé”™: {e}")
            return False

class FilterConfig:
    """è¿‡æ»¤é…ç½®ç®¡ç†ç±»"""
    
    @staticmethod
    def create_parser() -> argparse.ArgumentParser:
        """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
        parser = argparse.ArgumentParser(description='æ‰¹é‡å›¾ç‰‡è¿‡æ»¤å·¥å…·')
        parser.add_argument('--min_size', type=int, 
                        help=f'æœ€å°å›¾ç‰‡å°ºå¯¸(åƒç´ )')
        parser.add_argument('--enable_small_filter', action='store_true',
                        help='å¯ç”¨å°å›¾è¿‡æ»¤')
        parser.add_argument('--enable_grayscale_filter', action='store_true',
                        help='å¯ç”¨é»‘ç™½å›¾è¿‡æ»¤')
        parser.add_argument('--enable_duplicate_filter', action='store_true',
                        help='å¯ç”¨é‡å¤å›¾ç‰‡è¿‡æ»¤')
        parser.add_argument('--enable_text_filter', action='store_true',
                        help='å¯ç”¨çº¯æ–‡æœ¬å›¾ç‰‡è¿‡æ»¤')
        parser.add_argument('--merge_archives', action='store_true',
                        help='å¯ç”¨å‹ç¼©åŒ…åˆå¹¶å¤„ç†')
        parser.add_argument('--ref_hamming_threshold', type=int, 
                        help=f'å†…éƒ¨å»é‡çš„æ±‰æ˜è·ç¦»é˜ˆå€¼ ')
        parser.add_argument('--duplicate_filter_mode', type=str, default='quality',
                        choices=['quality', 'watermark', 'hash'],
                        help='é‡å¤å›¾ç‰‡è¿‡æ»¤æ¨¡å¼ (quality, watermark æˆ– hash)')
        parser.add_argument('--hash_file', type=str,
                        help='å“ˆå¸Œæ–‡ä»¶è·¯å¾„')
        parser.add_argument('--max_workers', type=int,
                        help='æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°')
        parser.add_argument('--clipboard', '-c', action='store_true',
                        help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
        parser.add_argument('paths', nargs='*', help='è¾“å…¥è·¯å¾„')
        return parser
    
    @staticmethod
    def build_filter_params(args) -> Dict[str, Any]:
        """ä»å‘½ä»¤è¡Œå‚æ•°æ„å»ºè¿‡æ»¤å‚æ•°å­—å…¸"""
        params = {
            'min_size': args.min_size,
            'enable_small_filter': args.enable_small_filter,
            'enable_grayscale_filter': args.enable_grayscale_filter,
            'enable_duplicate_filter': args.enable_duplicate_filter,
            'enable_text_filter': args.enable_text_filter,
            'duplicate_filter_mode': args.duplicate_filter_mode,
            'merge_archives': args.merge_archives,
            # æ³¨æ„è¿™é‡Œçš„å‚æ•°åä¿æŒä¸ImageFilterä¸€è‡´
            'ref_hamming_threshold': args.ref_hamming_threshold,
            'hash_file': args.hash_file,
            'max_workers': args.max_workers if args.max_workers else os.cpu_count() * 2,  # é»˜è®¤CPUæ ¸å¿ƒæ•°2å€
            'config': args  # ä¿ç•™åŸå§‹å‚æ•°å¯¹è±¡ä»¥å…¼å®¹ç°æœ‰ä»£ç 
        }
        
        # å¤„ç†æ°´å°å…³é”®è¯åˆ—è¡¨
        # if hasattr(args, 'watermark_keywords') and args.watermark_keywords:
        #     params['watermark_keywords'] = [kw.strip() for kw in args.watermark_keywords.split(',')]
            
        return params
    
    @staticmethod
    def get_preset_configs() -> Dict[str, Dict[str, Any]]:
        """è·å–é¢„è®¾é…ç½®"""
        return {
            "å»å°å›¾": {
                "description": "ä»…å»é™¤å°å°ºå¯¸å›¾ç‰‡",
                "checkbox_options": ["enable_small_filter", "clipboard"],
                "input_values": {
                    "min_size": str(DEFAULT_MIN_SIZE) 
                }
            },
            "å»é‡å¤": {
                "description": "ä»…å»é™¤é‡å¤å›¾ç‰‡",
                "checkbox_options": ["enable_duplicate_filter", "clipboard"],
                "input_values": {
                    "ref_hamming_threshold": str(DEFAULT_HAMMING_DISTANCE),
                    "duplicate_filter_mode": "quality"
                }
            },
            "å»æ°´å°å›¾": {
                "description": "å»é™¤å¸¦æ°´å°çš„å›¾ç‰‡",
                "checkbox_options": ["enable_duplicate_filter", "clipboard"],
                "input_values": {
                    "ref_hamming_threshold": str(DEFAULT_HAMMING_DISTANCE),
                    "duplicate_filter_mode": "watermark",
                }
            },
            "å»é»‘ç™½": {
                "description": "ä»…å»é™¤é»‘ç™½/ç™½å›¾",
                "checkbox_options": ["enable_grayscale_filter", "clipboard"],
            },
            "å“ˆå¸Œæ¯”å¯¹": {
                "description": "ä½¿ç”¨å“ˆå¸Œæ–‡ä»¶æ¯”å¯¹å»é‡",
                "checkbox_options": ["enable_duplicate_filter", "clipboard"],
                "input_values": {
                    "duplicate_filter_mode": "hash",
                    "hash_file": "",
                    "ref_hamming_threshold": str(DEFAULT_HAMMING_DISTANCE)
                }
            },
            "åˆå¹¶": {
                "description": "åˆå¹¶å¤šä¸ªå‹ç¼©åŒ…å¹¶å¤„ç†",
                "checkbox_options": ["merge_archives", "enable_duplicate_filter","clipboard"],
                "input_values": {
                    "duplicate_filter_mode": "quality",
                    "ref_hamming_threshold": str(1)
                }
            },
            "å®Œæ•´è¿‡æ»¤": {
                "description": "å®Œæ•´è¿‡æ»¤(å»é‡+å»å°å›¾+å»é»‘ç™½+å»æ–‡æœ¬)",
                "checkbox_options": ["merge_archives", "enable_small_filter", "enable_duplicate_filter", "enable_grayscale_filter", "clipboard"],
                "input_values": {
                    "min_size": str(DEFAULT_MIN_SIZE),
                    "ref_hamming_threshold": str(DEFAULT_HAMMING_DISTANCE),
                    "duplicate_filter_mode": "quality",
                }
            }
        }
class FilterProcessor:
    """è¿‡æ»¤å¤„ç†ç±»"""
    
    @staticmethod
    def process_group(paths: Set[str], filter_params: Dict[str, Any]) -> bool:
        """å¤„ç†å•ä¸ªè·¯å¾„ç»„
        
        Args:
            paths: è·¯å¾„é›†åˆ
            filter_params: è¿‡æ»¤å‚æ•°
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨åˆå¹¶æ¨¡å¼
        if filter_params.get('merge_archives', False):
            # åˆå¹¶æ¨¡å¼å¤„ç†
            return FilterProcessor._process_with_merge(paths, filter_params)
        else:
            # å•ç‹¬å¤„ç†æ¨¡å¼
            return FilterProcessor._process_individually(paths, filter_params)
    
    @staticmethod
    def _process_with_merge(paths: Set[str], filter_params: Dict[str, Any]) -> bool:
        """åˆå¹¶æ¨¡å¼å¤„ç†å¤šä¸ªå‹ç¼©åŒ…
        
        Args:
            paths: è·¯å¾„é›†åˆ
            filter_params: è¿‡æ»¤å‚æ•°
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        import shutil
        
        # åˆ›å»ºå›¾ç‰‡è¿‡æ»¤å™¨å’Œå‹ç¼©åŒ…å¤„ç†å™¨å®ä¾‹
        archive_handler = ArchiveHandler()
        archive_merger = ArchiveMerger()
        
        logger.info("[#update_log]å¯ç”¨åˆå¹¶æ¨¡å¼å¤„ç†å¤šä¸ªå‹ç¼©åŒ…")
        # å°†pathsè½¬æ¢ä¸ºåˆ—è¡¨
        paths_list = list(paths)
        
        # åˆå¹¶å‹ç¼©åŒ…
        temp_dir, merged_zip, original_archives = archive_merger.merge_archives(paths_list)
        
        if not temp_dir or not merged_zip:
            logger.error("[#update_log]å‹ç¼©åŒ…åˆå¹¶å¤±è´¥ï¼Œå°†å›é€€åˆ°å•ç‹¬å¤„ç†æ¨¡å¼")
            return FilterProcessor._process_individually(paths, filter_params)
            
        try:
            # å¤„ç†åˆå¹¶åçš„å‹ç¼©åŒ…
            logger.info(f"[#cur_progress]å¤„ç†åˆå¹¶å‹ç¼©åŒ…: {merged_zip}")
            success, error_msg, results = archive_handler.process_archive(merged_zip, filter_params)
            
            if not success:
                logger.error(f'[#update_log]å¤„ç†åˆå¹¶å‹ç¼©åŒ…å¤±è´¥: {error_msg}')
                return False
                
            # æ‹†åˆ†å¤„ç†åçš„å‹ç¼©åŒ…å›åŸå§‹å‹ç¼©åŒ…
            logger.info(f"[#cur_progress]æ­£åœ¨æ‹†åˆ†å¤„ç†åçš„å‹ç¼©åŒ…...")
            split_success = archive_merger.split_merged_archive(
                merged_zip, original_archives, temp_dir, filter_params
            )
            
            if not split_success:
                logger.error("[#update_log]æ‹†åˆ†åˆå¹¶å‹ç¼©åŒ…å¤±è´¥")
                return False
                
            logger.info("[#cur_stats]åˆå¹¶å¤„ç†æ¨¡å¼å®Œæˆ")
            for result in results:
                logger.info(f"[#file_ops]{result}")
                
            return True
            
        except Exception as e:
            logger.error(f"[#update_log]åˆå¹¶æ¨¡å¼å¤„ç†å‡ºé”™: {e}")
            return False
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if os.path.exists(merged_zip):
                    os.remove(merged_zip)
                    logger.info(f"[#file_ops]å·²åˆ é™¤ä¸´æ—¶å‹ç¼©åŒ…: {merged_zip}")
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir)
                    logger.info(f"[#file_ops]å·²åˆ é™¤ä¸´æ—¶ç›®å½•: {temp_dir}")
            except Exception as e:
                logger.error(f"[#update_log]æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    @staticmethod
    def _process_individually(paths: Set[str], filter_params: Dict[str, Any]) -> bool:
        """å•ç‹¬å¤„ç†æ¯ä¸ªè·¯å¾„
        
        Args:
            paths: è·¯å¾„é›†åˆ
            filter_params: è¿‡æ»¤å‚æ•°
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        # åˆ›å»ºå›¾ç‰‡è¿‡æ»¤å™¨å’Œå‹ç¼©åŒ…å¤„ç†å™¨å®ä¾‹
        archive_handler = ArchiveHandler()
        
        # å¤„ç†ç»“æœæ”¶é›†
        all_results = []
        process_failed = False
        
        # ç»Ÿè®¡æ€»æ•°
        total_paths = len(paths)
        completed = 0

        # å¤„ç†æ¯ä¸ªè·¯å¾„ï¼ˆæ­¤æ—¶pathså·²ç»æ˜¯å‹ç¼©åŒ…è·¯å¾„ï¼Œä¸å†éœ€è¦ç›®å½•éå†ï¼‰
        for path in paths:
            completed += 1
            progress_percent = int((completed / total_paths) * 100)
            logger.info(f"[@cur_stats]å¤„ç†è¿›åº¦ ({completed}/{total_paths}) {progress_percent}%")
            logger.info(f"[#cur_progress]å¤„ç†: {path}")
            
            # ç”±äºè·¯å¾„å·²ç»æ˜¯å‹ç¼©åŒ…æ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥è°ƒç”¨process_archive
            if Path(path).is_file() and Path(path).suffix.lower() in SUPPORTED_ARCHIVE_FORMATS:
                success, error_msg, results = archive_handler.process_archive(path, filter_params)
            else:
                # å¯¹äºä¸ç¬¦åˆæ¡ä»¶çš„è·¯å¾„ï¼Œä½¿ç”¨åŸæ¥çš„process_directoryå¤„ç†
                success, error_msg, results = archive_handler.process_directory(path, filter_params)
            
            if not success:
                logger.error(f'[#update_log]å¤„ç†å¤±è´¥ {path}: {error_msg}')
                process_failed = True
            else:
                for result in results:
                    logger.info(f"[#file_ops]{result}")
                all_results.extend(results)
                
        return not process_failed    
    @staticmethod
    def merge_results(paths: Set[str], results: List) -> bool:
        """åˆå¹¶å¤„ç†ç»“æœ
        
        Args:
            paths: æºè·¯å¾„é›†åˆ
            results: å¤„ç†ç»“æœ
            
        Returns:
            bool: åˆå¹¶æ˜¯å¦æˆåŠŸ
        """
        merger = ArchiveMerger()
        try:
            # é€‰æ‹©ä¿å­˜ç›®å½•(å•ä¸ªç›®å½•æˆ–ç¬¬ä¸€ä¸ªå‹ç¼©åŒ…æ‰€åœ¨ç›®å½•)
            save_dir = next(iter(paths))
            if len(paths) > 1:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå‹ç¼©åŒ…æ‰€åœ¨çš„ç›®å½•
                save_dir = os.path.dirname(save_dir)
                
            merger.merge_archives(results, save_dir)
            logger.info(f"[#file_ops]åˆå¹¶å¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {save_dir}")
            return True
        except Exception as e:
            logger.error(f"[#update_log]åˆå¹¶å¤„ç†å¤±è´¥: {str(e)}")
            return False

class Application:
    """æ‰¹é‡å›¾ç‰‡è¿‡æ»¤å·¥å…·åº”ç”¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–åº”ç”¨"""
        # æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„
        sys.path.append(os.path.dirname(os.path.dirname(__file__)))
        # åˆå§‹åŒ–TextualLogger
    
    def process_with_args(self, args) -> bool:
        """å¤„ç†å‘½ä»¤è¡Œå‚æ•°
        
        Args:
            args: å‘½ä»¤è¡Œå‚æ•°
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """

        # è·å–è¾“å…¥è·¯å¾„
        paths = InputHandler.get_input_paths(
            cli_paths=args.paths,
            use_clipboard=args.clipboard,
            allow_manual=True
        )
        if not paths:
            logger.error('[#update_log]æœªæä¾›ä»»ä½•è¾“å…¥è·¯å¾„')
            return False
        
        if HAS_TUI:
            initialize_textual_logger()
        # æ„å»ºè¿‡æ»¤å‚æ•°å­—å…¸
        filter_params = FilterConfig.build_filter_params(args)

        # å°†è·¯å¾„åˆ†ç»„å¤„ç†
        path_groups = InputHandler.group_input_paths(paths)
        overall_success = True
        
        # æ·»åŠ æ€»ä½“è¿›åº¦ç»Ÿè®¡
        total_groups = len(path_groups)
        
        for i, group in enumerate(path_groups):
            group_progress = int(((i + 1) / total_groups) * 100)
            logger.info(f"[@cur_stats]è·¯å¾„ç»„å¤„ç† ({i+1}/{total_groups}) {group_progress}%")
            logger.info(f"[#cur_progress]å¤„ç†è·¯å¾„ç»„ {i+1}/{total_groups}: {group}")
            success = FilterProcessor.process_group(group, filter_params)
            overall_success = overall_success and success
            
        # å¤„ç†å®Œæˆæç¤º
        if overall_success:
            logger.info("[#update_log]âœ… æ‰€æœ‰å¤„ç†ä»»åŠ¡å·²å®Œæˆ")
        else:
            logger.info("[#update_log]âš ï¸ å¤„ç†å®Œæˆï¼Œä½†æœ‰éƒ¨åˆ†ä»»åŠ¡å¤±è´¥")
            
        return overall_success
    
    def run_tui_mode(self):
        """è¿è¡ŒTUIç•Œé¢æ¨¡å¼"""
        parser = FilterConfig.create_parser()
        preset_configs = FilterConfig.get_preset_configs()

        def on_run(params: dict):
            """TUIé…ç½®ç•Œé¢çš„å›è°ƒå‡½æ•°"""
            # å°†TUIå‚æ•°è½¬æ¢ä¸ºå‘½ä»¤è¡Œå‚æ•°æ ¼å¼
            sys.argv = [sys.argv[0]]
            
            # æ·»åŠ é€‰ä¸­çš„å¤é€‰æ¡†é€‰é¡¹
            for arg, enabled in params['options'].items():
                if enabled:
                    sys.argv.append(arg)
                    
            # æ·»åŠ è¾“å…¥æ¡†çš„å€¼
            for arg, value in params['inputs'].items():
                if value.strip():
                    sys.argv.append(arg)
                    sys.argv.append(value)
            
            # ä½¿ç”¨å…¨å±€çš„ parser è§£æå‚æ•°
            args = parser.parse_args()
            self.process_with_args(args)

        # åˆ›å»ºé…ç½®ç•Œé¢
        app = create_config_app(
            program=__file__,
            parser=parser,
            title="å›¾ç‰‡è¿‡æ»¤å·¥å…·",
            preset_configs=preset_configs,
            # on_run=on_run
            # on_run=False
        )
        
        # è¿è¡Œé…ç½®ç•Œé¢
        app.run()
    
    def main(self) -> int:
        """ä¸»å‡½æ•°å…¥å£
        
        Returns:
            int: é€€å‡ºä»£ç (0=æˆåŠŸ)
        """
        try:
            parser = FilterConfig.create_parser()
            
            # å‘½ä»¤è¡Œæ¨¡å¼å¤„ç†
            if len(sys.argv) > 1:
                args = parser.parse_args()
                success = self.process_with_args(args)
                return 0 if success else 1
                
            # TUIæ¨¡å¼å¤„ç†
            else:
                self.run_tui_mode()
                return 0
                
        except Exception as e:
            logger.exception(f"[#update_log]é”™è¯¯ä¿¡æ¯: {e}")
            return 1

# ä¸»ç¨‹åºå…¥å£
if __name__ == '__main__':
    app = Application()
    sys.exit(app.main())